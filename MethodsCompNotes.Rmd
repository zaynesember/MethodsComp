---
title: "Methods Comp Notes"
author: "Zayne Sember"
date: "3/29/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(haven)
library(plotness) # can install with devtools::install_github("https://github.com/zaynesember/plotness")
library(vcd)
library(AER)
library(jtools)
library(cvTools)
library(separationplot)
library(kableExtra)
library(ROCR)
library(boot)
library(MASS)
library(olsrr)
library(car)
library(sjPlot)
library(tidyverse)
library(fixest)
library(modelsummary)
library(ggeffects)
```

# POLI 204B

## Hypothesis Testing

1.  Set some threshold, $\alpha$, at or below which you deem a p-value significant

2.  Define the null hypothesis, $H_0$

3.  Define the alternative hypothesis, $H_A$, either two-tailed or one-tailed

4.  Calculate a test statistic such as Z-score or t-test and check the p-value

5.  Draw conclusions

Some hypothesis tests in R

```{r}
# ONE SAMPLE T-TEST
# How likely is it that the mean of some population from which we take a normal sample, x, is greater than some number, mu?

x <- rnorm(100)
t.test(x, mu=5)

# WELCH TWO-SAMPLE T-TEST
# How likely is it that the means of two populations differ based on two normal samples of equal variance, x and y?

x <- rnorm(100)
y <- rnorm(100)

t.test(x, y)

# TWO PROPORTION Z-TEST
# Is there a significant difference in proportions between two populations given normal samples of them, x and y?

x <- rnorm(100)
y <- rnorm(100)

# Assuming we want to test whether the share of values greater than zero differs between the two populations
x_count <- length(x[x>0])
y_count <- length(y[y>0])

prop.test(x=c(x_count,y_count), n=c(length(x), length(y)), alternative="two.sided")


# CHI-SQUARED TEST
# Are two variables independent? Useful for testing relationships between categorical variables

#  For example, if you were investigating the relationship between occupation and party preference, and 35% of voters were Democrats, 32% were Independents, and 33% were Republicans, independence implies the same partisan breakdown across all occupational categories (35, 32, and 33).

data_frame <- read.csv("https://goo.gl/j6lRXD")

table(data_frame$treatment, data_frame$improvement)

chisq.test(data_frame$treatment, data_frame$improvement, correct=FALSE)


# DIFFERENCE BETWEEN VARIANCES TEST
# Is the variance between two normal samples, x and y, drawn from populations significantly different?

x <- rnorm(100, sd=3)
y <- rnorm(121, sd=3.5)

var.test(x, y)
```

## Properties of Estimators

### Unbiasedness

$E(\hat{\theta}) - \theta = 0$

### Asymptotic Unbiasedness

As $n$ gets bigger, the estimator's biasedness goes to zero $\lim_{n\to\infty} P(|E[\hat{\theta}] - \theta| > \epsilon) \to 0; \forall\epsilon >0$

### Efficiency

The estimator needs fewer observations to achieve better error performance $\frac{1}{MSE}=\frac{1}{E[(\hat{\theta}-\theta)^2]}$

### Consistency

The distribution of estimates converges on the true value as $n$ gets bigger $\lim_{n\to\infty} P(|\hat{\theta} - \theta| > \epsilon) \to 0; \forall\epsilon >0$

## Correlation

We define the correlation, $r$, between two variables, $x$ and $y$, to be:

$$r=\frac{\sum{Z_{xi}Z_{yi}}}{n-1}=\frac{\sum{(x_i-\bar{x})(y_i-\bar{y})}}{s_xs_y(n-1)}$$ where the standard deviation of $x$ (and same for $y$) is:

$$s_x=\sqrt{\frac{\sum{(x_i-\bar{x})^2}}{n-1}}$$

```{r}
x <- rnorm(n=100, mean=2, sd=3)
y <- rnorm(n=100, mean=-3, sd=2)

cor(x,y)
```

## Regression Basics

Ordinary least squares regression seeks to minimize the the vertical distances, $\epsilon$, between the data being fitted and some line of best fit with some slope (in the bivariate case), $\beta_1$, and some intercept, $\beta_0$.

The model we want to fit to:

$$y_i=\beta_0+\beta_1x_i+\epsilon_i$$

The model we estimate:

$$y_i=\hat{\beta_0}+\hat{\beta_1}x_i+e_i$$ The residuals, $e_i$, are an estimate of the error (distance between true line and y_i).

The residual sum of squares (RSS) is defined as: $$RSS=\sum e_i^2=\sum(y_i-\hat{y_i})^2$$

```{r}
# If we have some lm model we can get the RSS as follows:
x <- rnorm(n=100, mean=2, sd=3)
y <- rnorm(n=100, mean=-3, sd=2)

model <- lm(y ~ x)

deviance(model)

# or

sum(resid(model)^2)

# Or without an lm model:

rss <- function(y, y_hat){
  return(sum((y-y_hat)^2))
}

y_hat <- fitted(model)

rss(y, y_hat)
```

The regression sum of squares (RSS) (AKA sum of squares of regression (SSR), explained sum of squares (ESS)) is defined as: $$RegSS=\sum(\hat{y_i}-\bar{y})^2$$

```{r}
x <- rnorm(n=100, mean=2, sd=3)
y <- rnorm(n=100, mean=-3, sd=2)

model <- lm(y ~ x)

y_hat <- fitted(model)

regSS <- function(y_hat, y){
  return(sum((y_hat-mean(y))^2))
}

regSS(y_hat, y)
```

The total sum of squares (TSS) is defined as: $$TSS=\sum(y_i-\bar{y})^2$$

```{r}
y <- rnorm(n=100, mean=-3, sd=2)

TSS <- function(y){
  return(sum((y-mean(y))^2))
}

TSS(y)
```

The ratio of RegSS to TSS is our $r^2$, the share of variance in $y$ that is explained by $x$: $$r^2=\frac{RegSS}{TSS}=\frac{\sum(\hat{y_i}-\bar{y})^2}{\sum(y_i-\bar{y})^2}$$

```{r}
x <- rnorm(n=100, mean=2, sd=3)
y <- rnorm(n=100, mean=-3, sd=2)

model <- lm(y ~ x)

summary(model)$r.squared

# or

y_hat <- fitted(model)

rsquared <- function(y, y_hat){
  return(sum((y_hat-mean(y))^2)/sum((y-mean(y))^2))
}

rsquared(y, y_hat)
```

### Calculating bivariate regression by hand (linear method)

#### Step 1

Given the vectors $X$ and $Y$, find $X^2$, $XY$, and the sums of all of the preceding vectors

```{r}
calc_x2xy <- function(x, y){
  df <- data.frame(x=x,y=y)
  
  df$xsquared <- x^2
  df$xy <- x*y
  
  totals <- c(sum(x), sum(y), sum(df$xsquared), sum(df$xy))
  
  df <- rbind(df, totals)
  rownames(df) <- c(1:length(x),"total")
  
  return(df)
}

x <- rnorm(n=7, mean=2, sd=3)
y <- rnorm(n=7, mean=-3, sd=2)

df_step1 <- calc_x2xy(x=x,y=y)
df_step1
```

#### Step 2

Calculate $\hat\beta_1$ and $\hat\beta_0$: $$\hat\beta_1=\frac{N\sum x_iy_i-\sum x_i\sum y_i}{N\sum x_i^2-(\sum x_i)^2}$$ $$\hat\beta_0=\frac{\sum y_i -\hat\beta_1\sum x_i}{N}$$

```{r}
calc_beta1 <- function(df){
  totals <- tail(df,n=1)
  N <- length(df[[1]])-1
  
  df <- head(df,n=N)
  N <- length(df$x-1) # -1 bc row totals amended
  
  beta1 <- (N*totals$xy - (totals$x*totals$y))/((N*totals$xsquared)-(totals$x)^2)
  return(beta1)
}

calc_beta0 <- function(df, beta1){
  totals <- tail(df,n=1)
  N <- length(df[[1]])-1
  
  beta0 <- (totals$y - (beta1*totals$x))/N
  return(beta0)
}

beta1 <- calc_beta1(df=df_step1)
beta1

beta0 <- calc_beta0(df=df_step1, beta1=beta1)
beta0

# check with:
# lm(y~x)
```

#### Step 3

Next, calculate $x-\bar x$, $(x- \bar x)^2$, $y-\bar y$, $(y-\bar y)^2$, and $(x-\bar x)(y-\bar y)$.

```{r}
calc_r_table <- function(x, y){
  N <- length(x)
  df <- data.frame(x=x,y=y)
  
  df$x_minus_xbar <- df$x -  mean(x)
  df$x_minus_xbar_sq <- (df$x_minus_xbar)^2
  
  df$y_minus_ybar <- df$y -  mean(y)
  df$y_minus_ybar_sq <- (df$y_minus_ybar)^2
  
  df$x_minus_xbar_y_minus_ybar <- df$x_minus_xbar*df$y_minus_ybar
  
  return(df)
}

calc_rsquared <- function(x, y){
  df <- calc_r_table(x,y)
  N <- length(x)
  
  sd_x <- sqrt((sum(df$x_minus_xbar_sq))/(N-1))
  sd_y <- sqrt((sum(df$y_minus_ybar_sq))/(N-1))
  
  r <- (sum(df$x_minus_xbar_y_minus_ybar))/((N-1)*sd_x*sd_y)
  
  return(r^2)
}

calc_r_table(x=x,y=y)

calc_rsquared(x=x,y=y)

# check with:
#summary(lm(y~x))$r.squared
```

#### Step 4

Calculate the residuals, their square, and from that the estimated standard error $$e_i=y_i-\hat y_i=y_i-\hat\beta_0-\hat\beta_1 x_i$$ $$\hat\sigma=\sqrt{\frac{\sum e_i^2}{N-2}}$$

```{r}
calc_residuals <- function(x, y, beta0, beta1){
  df <- data.frame(e=y-beta0-(beta1*x))
  df$e_squared <- df$e^2
  
  return(df)
}

calc_sigma <- function(e_squared, N){
  return(sqrt(sum(e_squared)/(N-2)))
}

residuals <- calc_residuals(x, y, beta0, beta1)
residuals

# check with:
#resid(lm(y~x))

sigma <- calc_sigma(residuals$e_squared, N=length(x))
sigma

# check with:
#summary(lm(y~x))$sigma


```

### Calculating bivariate regression by hand (matrix method)

Just calculate $\hat\beta=(X'X)^{-1}X'Y$

```{r}
x <- rnorm(n=7, mean=2, sd=3)
y <- rnorm(n=7, mean=-3, sd=2)

print("Append ones to X")
X <- matrix(c(rep(1, length(x)),x), nrow=length(x))
X

print("Define Y")
Y <- as.matrix(y)
Y

print("Get X'X")
t(X) %*% X

print("Get the inversion of X'X")
solve(t(X) %*% X)

print("Get X'Y")
t(X) %*% Y

print("Get beta")
solve(t(X) %*% X) %*% t(X) %*% Y

# check with:
#lm(y~x)
```

## Multiple Regression

We now have multiple $X$s we need to fit to:

$$Y=\beta_0+\beta_1X_1+\beta_2X_2+\epsilon$$

We want to find the values of $\beta_0$, $\beta_1$, and $\beta_2$ that **minimizes** the sum of squared residuals (RSS):

$$RSS=\sum(\hat{y_i}-\bar{y})^2=\sum(\hat{y_i}-(\hat{\beta_0}+\hat\beta_1X_1+\hat\beta_2X_2)^2$$

## Categorical Variables

When regressing on a categorical variable you leave one category out. Coefficients are the impact of being $X$ vs. the excluded category, all else equal.

## Fixed Effects

Suppose we have data on 100 individuals in each of 100 countries ($n=10,000$).

If we include a dummy to "soak up" the country effects we can "isolate" the differences between the individuals across countries. - The estimated coefficients for each country typically aren't reported bc they aren't relevant to our RQ

```{r}
data(trade)

# Without fixed effects
ols <- lm(log(Euros) ~ log(dist_km), data=trade)

summary(ols)

# With fixed effects by country of origin and destination, product, and year
fe_ols <- feols(log(Euros) ~ log(dist_km) | Origin + Destination + Product + Year, trade)

summary(fe_ols)
```

## Interactions

Case 1: Interacting a dummy with another dummy - Coefficient is the (estimated) change in the *intercept* of the line of best fit between units where both dummies are zero vs. where they are both 1.

Case 2: Interacting a dummy with a continuous variable - Coefficient is the difference in coefficient on the continuous variable when dummy is on vs. off.

Case 3: Interacting a continuous variable with another continuous variable. - Don't try to interpret coefficient, look at an interaction plot

Overall, an interaction tells us the *impact* one variable has on another variable's effect on the outcome. - Only include them when you have a theoretical reason for doing so

### Interaction plots

```{r}
set.seed(10)

#create data frame
df <- data.frame(gender = c(0,0,0,1,0,1,1,0,1,0),
                 exercise = c(0,0,2,1,2,0,1,0,2,1),
                 weight_loss = rnorm(n=10,mean=2,sd=3)) %>% 
  mutate(motivation = (2.5*weight_loss + rnorm(n=10, mean=3, sd=3)))
  
head(df)

model <- lm(weight_loss ~ gender*exercise, data=df)
summary(model)

# Plot interaction of gender*exercise
ggplot(data=df, aes(x=exercise, y=weight_loss, color=as.factor(gender))) +
  geom_point() +
  geom_smooth(method="lm") +
  labs(title="Interaction between gender and exercise") +
  theme_light()


model <- lm(weight_loss ~ exercise*motivation, data=df)
summary(model)

# To visualize an interaction between continuous variables you have to discretize
# one of them, i.e. group those above and below the mean
df$motivation_groups <- case_when(
  df$motivation > mean(df$motivation)+sd(df$motivation) ~ "high",
  df$motivation < mean(df$motivation)+sd(df$motivation) & df$motivation > mean(df$motivation)-sd(df$motivation) ~ "medium",
  df$motivation < mean(df$motivation)-sd(df$motivation) ~ "low"
)

# Can plot the interactions manually

ggplot(data=df, aes(x=exercise, y=weight_loss, color=as.factor(motivation_groups))) +
  geom_point() +
  geom_smooth(method="lm") +
  labs(title="Interaction between motivation and exercise") +
  theme_light()

# OR with plot_model
plot_model(model, type = "pred", terms = c("exercise", "motivation")) +
  theme_light()

plot_model(model, type = "int") +
  theme_light()
```

## Regression Presentation

### Regression tables

Pretty self-explanatory

### Coefficient Plots

Easier to read than a Big Ugly Table of Numbers (BUTON) - Always include 95% CI

```{r}
# Source: https://bookdown.org/paul/applied-data-visualization/graph-coefficient-plots.html

fit <- lm(Fertility ~ Catholic + Agriculture + Education, data = swiss) # see ?swiss
results <- tidy(fit)
fit_cis_95 <- confint(fit, level = 0.95) %>% 
  data.frame() %>%
  rename("conf.low_95" = "X2.5..",
         "conf.high_95" = "X97.5..")
fit_cis_90 <- confint(fit, level = 0.90) %>% 
  data.frame() %>%
  rename("conf.low_90" = "X5..",
         "conf.high_90" = "X95..")
results <- bind_cols(results, 
                     fit_cis_95, 
                     fit_cis_90) %>%
           rename(Variable = term,
                  Coefficient = estimate,
                  SE = std.error) %>%
           filter(Variable != "(Intercept)")


ggplot(results, aes(x = Variable, y = Coefficient)) +
        geom_hline(yintercept = 0, colour = gray(1/2), lty = 2) +
        geom_point(aes(x = Variable, 
                    y = Coefficient)) + 
        # geom_linerange(aes(x = Variable, 
        #              ymin = conf.low_90,
        #              ymax = conf.high_90),
        #            lwd = 1) +
        geom_linerange(aes(x = Variable, 
                     ymin = conf.low_95,
                     ymax = conf.high_95),
                   lwd = 1/2) + 
        ggtitle("Outcome: Fertility") +
        coord_flip() +
  theme_light()

# OR

coefplot(fit)
```

### Predicted Value Plots

These plots convey the substantive importance of your findings; the magnitude of the impact of your IVs on your DV

```{r}
df <- data.frame(x1=c(3, 4, 4, 5, 5, 6, 7, 8, 11, 12),
                 x2=c(6, 6, 7, 7, 8, 9, 11, 13, 14, 14),
                 x3=c(15, 12, 8, 9, 6, 5, 5, 2, 1, 0),
                 y=c(22, 24, 24, 25, 25, 27, 29, 31, 32, 36))

#fit multiple linear regression model
model <- lm(y ~ x1 + x2 +x3, data=df)

#plot predicted vs. actual values
ggplot(df, aes(x=predict(model), y=y)) + 
  geom_point() +
  geom_abline(intercept=0, slope=1) +
  labs(x='Predicted Values', y='Actual Values', 
       title='Predicted vs. Actual Values') +
  theme_light()
```

### Marginal Effects Plot

Another way to visualize the substantive meaning of your regression coefficients, especially when interactions are involved.

```{r}
data(efc)
model <- lm(barthtot ~ c12hour + neg_c_7 * c161sex + e42dep, data = efc)

ggpredict(model, terms = "c12hour")

df_predict <- ggpredict(model, terms = "c12hour")
ggplot(df_predict, aes(x, predicted)) +
  geom_line() +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .1)

# OR use plot_model
plot_model(model, type = "pred", terms = c("c12hour")) +
  theme_light()
```

## Regression Assumptions and Diagnostics

### Four assumptions of regression

1.  *Linearity*: The relationship between $X$ and the mean of $Y$ is linear

2.  *Homoscedasticity*: The variance of the residual is the same for any value of $X$

3.  *Independence*: Observations are independent of each other

4.  *Normality*: For any fixed value of $X$, $Y$ is normally distributed ## Regression Diagnostics ##\# Residual Plots Residuals are the variance in $Y$ that is unexplained by $X$ $$\hat{e_i}=Y_i-\hat{Y_i}$$

```{r}
fit <- lm(Fertility ~ Catholic + Agriculture + Education, data = swiss)


# Residuals vs. X
# Ideally want a "cloud" without a large amount of values at the extremes of X, we want it to look basically random

# If we have a U-shape or an inverted U then that indicates the data may be better suited for a nonlinear model

# We also want to check for any major outliers, is there a point that's super far away from the rest?
ggplot(data=fit, aes(x=Agriculture, y=.resid)) + 
  geom_point() +
  theme_light()
```

### Leverages

Leverages tell us how much influence each value of $X$ have

```{r}
fit <- lm(Fertility ~ Catholic + Agriculture + Education, data = swiss)

leveragePlot(model=fit, term.name="Catholic")
```

### Studentized and Standardized Residuals

Standardized measurements of residuals, helps you see outliers.

```{r}
fit <- lm(Fertility ~ Catholic + Agriculture + Education, data = swiss)

# Regular ole residuals
ggplot(data=fit, aes(x=.fitted, y=.resid)) +
  geom_point() +
  theme_light()

# Studentized residuals
ggplot(data=fit, aes(x=.fitted, y=rstudent(fit))) +
  geom_point() +
  theme_light()

# Standardized residuals
ggplot(data=fit, aes(x=.fitted, y=rstandard(fit))) +
  geom_point() +
  theme_light()
```

### DFBETA

Helps assess how influential each observation is. Essentially calculates how much the slope changes when an observation is removed from the model

```{r}
fit <- lm(Fertility ~ Catholic + Agriculture + Education, data = swiss)

ols_plot_dfbetas(fit)

# OR from car

dfbetasPlots(fit)
```

### Cook's Distance

Tells you how far, on average, the predicted y-values will move if an observation is dropped (note DFBeta is concerned with change in slope but also shows you outliers)

Especially helpful for multivariate regression b/c combos of $X$s could yield an influential point a residual plot won't catch.

```{r}
fit <- lm(Fertility ~ Catholic + Agriculture + Education, data = swiss)

ggplot(data=fit, aes(x=Catholic, y=cooks.distance(fit))) +
  geom_point() +
  theme_light()

ggplot(data=fit, aes(x=Agriculture, y=cooks.distance(fit))) +
  geom_point() +
  theme_light()
```

### Multicollinearity

The symptoms:

-   Instability of estimates when adding or dropping variables

-   Massive coefficients or t-statistics

-   Two or more variables that are similar but with opposite signs

What do to:

-   More data!

-   Simplify the model

    -   Do we need all the variables that are collinear?

    -   Could we combine two or more variables to create some sort of index?

## Instrumental Variables

A type of two-stage least squares regression (2SLS). Normally, when we want to test whether $X$ has some effect on $Y$ we regress $Y$ on $X$ ($Y\sim X$ AKA $Y=X\beta + \epsilon$). However, when $X$ is correlated with the error term (i.e. it is endogenous), $\epsilon$, OLS (and ANOVA) gives biased results. We get around this by using an instrumental variable, $Z$, which is *uncorrelated* with $\epsilon$ and affects $Y$ only through $X$ (i.e. is correlated with $X$ but not with $\epsilon$).

An example in `R`:

We want to estimate the relationship between the after-tax average price of a pack of cigarettes ($X$) on the number of cigarette packs sold per capita ($Y$). We can't estimate this relationship directly because there is simultaneous causality between supply and demand. However, we can use the sales tax per pack as an instrument ($Z$) as sales tax is correlated with the after-tax price of a pack ($X$) and presumably with the quantity of packs sold ($Y$) but only because sales tax impacts the price per pack.

```{r}
# Example from: https://www.econometrics-with-r.org/12.1-TIVEWASRAASI.html

data("CigarettesSW")

# Compute real per capita prices (X)
CigarettesSW$rprice <- with(CigarettesSW, price / cpi)

# Compute the sales tax (Z)
CigarettesSW$salestax <- with(CigarettesSW, (taxs - tax) / cpi)

# Check the correlation between Z and X
cor(CigarettesSW$salestax, CigarettesSW$price)

# Generate a subset for the year 1995
c1995 <- subset(CigarettesSW, year == "1995")

# Now we estimate the first stage X ~ Z
cig_s1 <- lm(log(rprice) ~ salestax, data = c1995)

summary(cig_s1)

# Now get the predicted values from our first stage regression
lcigp_pred <- cig_s1$fitted.values

# And regress our Y on our first stage fitted values
cig_s2 <- lm(log(c1995$packs) ~ lcigp_pred)

summary(cig_s2)

# NOTE: the AER package has a function that carries out 2SLS automatically
# ivreg(Y ~ X | Z, data)
cig_ivreg = ivreg(log(packs) ~ log(rprice) | salestax, data = c1995)

summary(cig_ivreg)

```

# POLI 271

## Likelihood Theory

### Deriving the log likelihood

Given some probability density function (our stochastic component), $f(y;\theta)$ (could be normal, Gaussian, etc.):

1.  Get the joint probability of the data. If the probabilities are independent then we have:

$$Pr(y_i,. . ., y_n|\theta_1, . . ., \theta_n)=Pr(y;\theta)=\prod_{i=1}^nf(y_i;\theta_i)$$

2.  Convert the joint probability to a likelihood. Take the joint probability, multiply it by some constant scalar, $h(y)$.

$$\mathcal{L}(\theta|y)=h(y)\times Pr(y;\theta)$$

3.  Plug in our $f(y;\theta)$ and simplify $\mathcal{L}(\theta|y)$

4.  Simplify further by taking the log of $\mathcal{L}(\theta|y)$

5.  Find the critical points by taking the first derivative w.r.t $\theta$

6.  Check whether they are maxima or minima by taking the second derivative w.r.t. $\theta$

An example:

The exponential probability density function is defined as: $f_e(x;\theta)=\theta e^{-\theta x}$.

First, note that since $X~f_e(x;\theta)$ the support (possible values of $X$) is $\mathbb{R}\in [0, \infty]$

1.  Get the joint probability

$$f_{\mathrm{joint}}(x,\theta)=\prod_{i=1}^n(\theta e^{-\theta x_i})=\theta^n\prod_{i=1}^n( e^{-\theta x_i})$$

2.  Define the likelihood function

$$\mathcal{L}(\theta|x)=h(x)\theta^n\prod_{i=1}^n( e^{-\theta x_i})$$

Let $h(x)=1$

$$\mathcal{L}(\theta|x)=\theta^n\prod_{i=1}^n( e^{-\theta x_i})$$

3.  Take the log

$$\ln(\mathcal{L}(\theta|x))=\ln(\theta^n\prod_{i=1}^n( e^{-\theta x_i}))$$ $$\ln(\mathcal{L}(\theta|x))=n\ln(\theta)-\theta\sum_{i=1}^nx_i$$

4.  Take the first derivative and identify the critical points

$$\frac{d\ln(\mathcal{L}(\theta|x))}{d\theta}=\frac{n}{\theta}-\sum_{i=1}^nx_i$$

Set the derivative equal to zero to find the critical values of $\theta$:

$$\frac{n}{\theta}-\sum_{i=1}^nx_i=0$$

$$\hat{\theta}=\frac{n}{\sum_{i=1}^nx_i}$$

5.  Take the second derivative and substitute $\hat{\theta}$ for $\theta$. If the result is positive the critical point is a minumum, if zero then it's a saddle, if negative it's a maximum.

$$\frac{d^2\ln(\mathcal{L}(\theta|x))}{d\theta^2}=\frac{-n}{\theta^2}$$

Plugging in $\hat{\theta}$:

$$\frac{d^2ln(\mathcal{L}(\theta=\hat{\theta}|x))}{d\theta^2}=\frac{-n\sum_{i=1}^nx_i}{n}=-\sum_{i=1}^nx_i$$

What's the sign of $-\sum_{i=1}^nx_i$?

We know $-\sum_{i=1}^nx_i<0$ because $\sum_{i=1}^nx_i>0$ given the $X$ is bounded between $[0, \infty]$ therefore it is a maximum and thus $\hat{\theta}$ is a maximum and the MLE is $\hat{\theta}=\frac{n}{\sum_{i=1}^nx_i}$.

Now in R:

```{r}
# Sets the seed for R's baked in random number generation for easy replication
set.seed(2)

# Assigns x to be a vector of 1000 observations drawn from the exponential distribution
# with lambda=3
x <- rexp(1000, 3)

# Returns the log likelihood for some parameter, theta, given data, x
exp.ll <- function(theta, x){return((length(x)*log(theta))-(theta*sum(x)))}

thetas = seq(0,25,0.1)

y <- c()

for(theta in thetas){
  y <- c(y,exp.ll(theta=theta,x=x))
}

theta_hat <- length(x)/sum(x)
paste("For the given x, theta_hat is: ", theta_hat)

df <- data.frame(y,thetas)

ggplot(data=df,aes(x=thetas,y=y)) +
  geom_point(size=0.5) +
  geom_vline(xintercept=theta_hat, color="red", linetype="dashed") +
  ggtitle("Log likelihood vs. theta") +
  xlab("theta") +
  ylab("Log likelihood") +
  theme_bw()
```

Note: See MLE Problem Set 2 for a derivation of the MLE of the normal variance.

## Regularity

In order to find the MLE as we did above, the likelihood function must demonstrate "regularity", i.e. it must be continuous and twice differentiable.

## Binary Data

### Why not just use OLS?

-   Nonsensical predictions: predicted probabilites using a linear probability model on a binary outcome can fall outside $[0,1]$, meaning the OLS estimate is biased and inconsistent.
-   Assumes the relationship between a covariate and the probability of 1 as the outcome is constant across all values of the covariate
-   Can give you heteroskedastic residuals that give biased confidence intervals

### The Logit Model

Logit allows for mapping probabilities onto the unbounded real line using odds. We transform a probability, $p$, to logit by taking the log of the odds ratio:

$$\mathrm{logit}(p)=\log(\frac{p}{1-p})$$

And the inverse mapping:\

$$
\mathrm{logit}^{-1}(x)=\frac{e^x}{1+e^x}=\frac{1}{1+e^{-x}}
$$

We model binary outcomes as Bernoulli trials so our stochastic component is:

$$
f_B(y_i;\theta_i)=\theta_i^{y_i}(1-\theta_i)^{1-y_i}= 
   \left\{
\begin{array}{ll}
      \theta_i & y_i=1 \\
      1-\theta_i & y_i=0
\end{array} 
\right. 
$$

And our systematic component is:

$$
\theta_i=\mathrm{logit}^{-1}(x_i^T\beta)=\frac{1}{1+e^{-x_i^T\beta}}
$$

As we do above we can then get the joint probability distribution:

$$
Pr(y|\theta)=\prod_{i=1}^n \theta_i^{y_i}(1-\theta_i)^{1-y_i}
$$

And the log-likelihood:

$$
\ln\mathcal{L}(\theta|y)=\sum_{i=1}^n\ln(\frac{e^{-x_i^T\beta^{1-y_i}}}{1+e^{-x_i^T\beta}})
$$

Finding the MLE with R (Ward and Ahlquist code):

```{r}
#function for optimization of logit
binreg<- function(X,y,method="BFGS"){
  X<- cbind(1,X)
  
  # Get the log-likelihood
  negLL<- function(b,X,y){
    p<-as.vector(1/(1+exp(-X %*% b)))
    
    return(-sum(y*log(p) + (1-y)*log(1-p)))
  }
  
  # Get the gradient (score function)
  gradient<- function(b,X,y){
    p <- as.vector(1/(1+exp(-X %*% b)))
    
    return(-apply(((y - p)*X), 2, sum))
  }
  
  # Pass our data, log-likelihood, and gradient to optim() 
  # which will find the maximum likelihood
  results<- optim(par=rep(0,ncol(X)), 
                  fn=negLL,
                  gr=gradient, 
                  hessian=T, 
                  method=method,
                  X=X,
                  y=y)
  
  # Pull out the stuff we want
  list(coefficients=results$par,
       stderr=sqrt(diag(solve(results$hessian))),
       var=solve(results$hessian),
       deviance=2*results$value,
       converged=results$convergence==0)
}

data("mtcars")

# set the DV to the binary variable for V-shaped vs. straight engine type
DV <- mtcars$vs

IVs <- as.matrix(mtcars %>% select(cyl, disp, hp))

mlebin.fit<-binreg(X=IVs, y=DV)

# Get our logit coefficients
round(mlebin.fit$coefficients,2)


# Compare to R's baked in logit estimation
fit.glm <- glm(vs ~ cyl + disp + hp,
               family = binomial(link = "logit"), 
               data = mtcars)
summary(fit.glm, signif.stars=FALSE)
```

### Interpreting Logit

While the output of logit is similar to OLS, interpretation is a bit more nuanced for 2 reasons:

1.  The model is nonlinear so the effect of a particular covariate on the DV is not constant across all levels of the covariate. This can be seen by deriving the marginal effect of a covariate, $x_k$, on the expected value of $Y$:

$$\frac{\partial E[Y_i]}{\partial x_{ki}}=\frac{\partial\theta_i}{\partial x_{ki}}=\beta_k\frac{e^{x_i^T\beta}}{(1+e^{x_i^T\beta})^2}$$

As a quick trick, you can divide a coefficient by 4 to get an estimate of a maximum effect a 1 unit increase in the covariate can have on the probability of the DV being 1. i.e. if we have a coefficent of -1.45 then $\frac{-1.45}{4}=-0.4$ so a 1 unit increase in the IV reduces the probability of the DV being one by about 40%.

2.  The logit model is a linear regression on the log odds so the exponentiated coefficients are odds ratios. If we have a coefficient greater than 1 then a 1 unit increase in that covariate corresponds to an *increase* in the relative probability of obtaining a 1 in the outcome variable. Less than 1 represents a decrease.

### Interpreting Logit with Plots

We can use a coefficient plot to avoid a BUTON

```{r}
# Read in and clean up the data
impeach <- read_csv("Data/impeach.csv")
#impeach <- impeach %>% select(-ccoal98)
impeach <- na.omit(impeach)
attach(impeach)

# Make our DV binary
impeach$votebin[votesum != 0] <- 1
impeach$votebin[votesum == 0] <- 0

# Estimate the model
model <- glm(votebin ~ partyid + clint96, family=binomial(link="logit"), data=impeach)

summary(model)

# Tidy the model
results <- tidy(model)

# Get the 95% CI
fit_cis_95 <- confint(model, level = 0.95) %>% 
  data.frame() %>%
  rename("conf.low_95" = "X2.5..",
         "conf.high_95" = "X97.5..")

# Get the 90% CI
fit_cis_90 <- confint(model, level = 0.90) %>% 
  data.frame() %>%
  rename("conf.low_90" = "X5..",
         "conf.high_90" = "X95..")

# Bind it together
results <- bind_cols(results, 
                     fit_cis_95, 
                     fit_cis_90) %>%
           rename(Variable = term,
                  Coefficient = estimate,
                  SE = std.error) %>%
           filter(Variable != "(Intercept)")

# Plot it!
ggplot(results, aes(x = Variable, y = Coefficient)) +
        geom_hline(yintercept = 0, color = "red", lty = 2) +
        geom_point(aes(x = Variable, 
                    y = Coefficient)) + 
        # geom_linerange(aes(x = Variable, 
        #              ymin = conf.low_90,
        #              ymax = conf.high_90),
        #            lwd = 1) +
        geom_errorbar(aes(x = Variable, 
                     ymin = conf.low_95,
                     ymax = conf.high_95),
                     width=.1) + 
        ggtitle("Coefficient Plot") +
        coord_flip() +
  theme_light()
```

We can use a predicted probability plot to show the substantive impact of our results

```{r}
# Say we want to know the difference in predicted probability
# of a Republican vs. a Democrat voting to impeach Clinton

# Create a sequence of the range of values for the 96 vote share
clint96.range <- seq(min(impeach$clint96),max(impeach$clint96),.1)

# Construct the Democratic scenario
x.lo <- c(1, 0, median(impeach$clint96)) # Democrat scenario
X.lo <- matrix(x.lo, nrow=length(x.lo), ncol=length(clint96.range))
X.lo[3,] <- clint96.range

# Construct the Republican scenario
x.hi <- c(1, 1, median(impeach$clint96)) # Republican scenario
X.hi <- matrix(x.hi, nrow=length(x.hi), ncol=length(clint96.range))
X.hi[3,] <- clint96.range

# Generate a multivariate normal distribution
B.tilde <- mvrnorm(1000, coef(model), vcov(model))

# Generate the predicted probabilities
s.lo <- inv.logit(B.tilde %*% X.lo)
s.hi <- inv.logit(B.tilde %*% X.hi)
s.lo <- apply(s.lo, 2, quantile, c(0.025, 0.5, 0.975))
s.hi <- apply(s.hi, 2, quantile, c(0.025, 0.5, 0.975))


# Now tidy the data and plot with ggplot
pred.prob.df <- data.frame(xrange=rep(clint96.range,2), 
                           y=c(s.lo[2,],s.hi[2,]),
                           conf.low = c(s.lo[1,], s.hi[1,]),
                           conf.high = c(s.lo[3,], s.hi[3,]),
                           scenario = rep(c("Democrat", "Republican"), 
                                          each=length(clint96.range)))

ggplot(data=pred.prob.df, aes(x=xrange, y=y,
                              ymin=conf.low,
                              ymax=conf.high,
                              fill=factor(scenario),
                              linetype=factor(scenario))) +
  geom_line(size=.8) +
  geom_ribbon(alpha=0.25, color="gray") +
  labs(x="1996 Vote Share for Clinton", y="Predicted Probability",
       title="Predicted Probability Plot", linetype="Scenario") +
  scale_fill_manual(labels=c("Democrat", "Republican"), 
                    values=c("blue", "red")) +
  theme_minimal() +
  guides(fill="none", color="none")

# Or use base graphics (ew)

#jpeg(file="pred_prob.jpeg")

# Get our blank plot
plot(clint96.range, s.lo[2,], ylim=c(0,1),
     xlab="Clinton 1996 District Vote Share",
     ylab="Predicted Probability of Voting for Impeachment",
     main="Impeachment Vote, Party ID, and 1996 Clinton Vote Share",
     bty="n", col="white")

# Plot the confidence bands
polygon(x=c(clint96.range, rev(clint96.range)),
        y=c(s.lo[1,],rev(s.lo[3,])),
        col=grey(0.8), border=NA)

polygon(x=c(clint96.range, rev(clint96.range)),
        y=c(s.hi[1,],rev(s.hi[3,])),
        col=grey(0.8), border=NA)

#Plot the predicted probability curves
lines(clint96.range, s.hi[2,], lty=3, lwd=2)

lines(clint96.range, s.lo[2,], lwd=2)

legend(70, 1, legend=c("Democrat", "Republican"), lty=c(1,3),lwd=3)
#dev.off()
```

We can also use a predicted probability plot to model an interaction in a logit model

```{r}
# Load in the data
impeach <- read_csv("Data/impeach.csv")
impeach <- impeach %>% na.omit()

impeach$votebin[impeach$votesum != 0] <- 1
impeach$votebin[impeach$votesum == 0] <- 0

model_int <- glm(votebin ~ clint96 * aflcio97 ,family=binomial(link="logit"),
                 data=impeach)
model_noint <- glm(votebin ~ clint96 + aflcio97,
                   family=binomial(link="logit"),
                   data=impeach)

modelsummary(list(model_noint, model_int), title="Impeachment models")
```

```{r}
clint96.range <- seq(quantile(impeach$clint96)[2],
                    quantile(impeach$clint96)[4],.1)

# NO INTERACTION MODEL
x.lo <- c(1, median(impeach$clint96), 0) # Q1 clint96 scenario
X.lo <- matrix(x.lo, nrow=length(x.lo), ncol=length(clint96.range))
X.lo[2,] <- clint96.range

x.hi <- c(1, median(impeach$clint96), 100) # Q2 clint96 scenario
X.hi <- matrix(x.hi, nrow=length(x.hi), ncol=length(clint96.range))
X.hi[2,] <- clint96.range

B.tilde <- mvrnorm(1000, coef(model_noint), vcov(model_noint))

s.lo <- inv.logit(B.tilde %*% X.lo) # Note we would use pnorm() here instead if 
s.hi <- inv.logit(B.tilde %*% X.hi) # evaluating a probit model!

s.lo <- apply(s.lo, 2, quantile, c(0.025, 0.5, 0.975))
s.hi <- apply(s.hi, 2, quantile, c(0.025, 0.5, 0.975))


# Now tidy the data and plot with ggplot
pred.prob.df.noint <- data.frame(xrange=rep(clint96.range,2), 
                           y=c(s.lo[2,],s.hi[2,]),
                           conf.low = c(s.lo[1,], s.hi[1,]),
                           conf.high = c(s.lo[3,], s.hi[3,]),
                           scenario = rep(c("AFL-CIO=100", "AFL-CIO=0"), 
                                          each=length(clint96.range)))

ggplot(data=pred.prob.df.noint, aes(x=xrange, y=y,
                              ymin=conf.low,
                              ymax=conf.high,
                              fill=factor(scenario),
                              linetype=factor(scenario))) +
  geom_line(size=.8) +
  geom_ribbon(alpha=0.25, color="gray") +
  labs(x="1996 Vote Share for Clinton", y="Predicted Probability",
       title="Predicted Probability Plot for No Interaction Model", linetype="Scenario") +
  scale_fill_manual(labels=c("AFL-CIO=100", "AFL-CIO=0"), 
                    values=c("blue", "red")) +
  theme_minimal() +
  guides(fill="none", color="none")


# INTERACTION MODEL
x.lo <- c(1, median(impeach$clint96)[2], 0, 0) # 0 AFL-CIO score scenario
X.lo <- matrix(x.lo, nrow=length(x.lo), ncol=length(clint96.range))
X.lo[2,] <- clint96.range
X.lo[4,] <- clint96.range*0

x.hi <- c(1, quantile(impeach$clint96)[4], 100, 0) # 100 AFL-CIO score scenario
X.hi <- matrix(x.hi, nrow=length(x.hi), ncol=length(clint96.range))
X.hi[2,] <- clint96.range
X.hi[4,] <- clint96.range*100

B.tilde <- mvrnorm(1000, coef(model_int), vcov(model_int))

s.lo <- inv.logit(B.tilde %*% X.lo)
s.hi <- inv.logit(B.tilde %*% X.hi)

s.lo <- apply(s.lo, 2, quantile, c(0.025, 0.5, 0.975))
s.hi <- apply(s.hi, 2, quantile, c(0.025, 0.5, 0.975))


# Now tidy the data and plot with ggplot
pred.prob.df.int <- data.frame(xrange=rep(clint96.range,2), 
                           y=c(s.lo[2,],s.hi[2,]),
                           conf.low = c(s.lo[1,], s.hi[1,]),
                           conf.high = c(s.lo[3,], s.hi[3,]),
                           scenario = rep(c("AFL-CIO=100", "AFL-CIO=0"), 
                                          each=length(clint96.range)))

ggplot(data=pred.prob.df.int, aes(x=xrange, y=y,
                              ymin=conf.low,
                              ymax=conf.high,
                              fill=factor(scenario),
                              linetype=factor(scenario))) +
  geom_line(size=.8) +
  geom_ribbon(alpha=0.25, color="gray") +
  labs(x="1996 Vote Share for Clinton", y="Predicted Probability",
       title="Predicted Probability Plot for Interaction Model", linetype="Scenario") +
  scale_fill_manual(labels=c("AFL-CIO=100", "AFL-CIO=0"), 
                    values=c("blue", "red")) +
  theme_minimal() +
  guides(fill="none", color="none")
```

### Comparing Models

### ROC Plots

Use these to assess model fit heuristically. We want our model's curve to be as close to a straight diagonal line as possible.

In the below example, because the simpler model performs about as well as the more complicated one we would opt for the simpler model (assuming AIC, BIC, etc. are similar)

Note: We can also calculate and compare the Area Under the Curve (AUC) for each model's ROC curve; smaller is better.

```{r}
# Read in and clean up the data
impeach <- read_csv("Data/impeach.csv")
impeach <- na.omit(impeach)
attach(impeach)

# Make our DV binary
impeach$votebin[votesum != 0] <- 1
impeach$votebin[votesum == 0] <- 0

# Define two models to compare, one without aflcio97, one with
model1 <- glm(votebin ~ partyid + clint96, family=binomial(link="logit"), data=impeach)

model2 <- glm(votebin ~ partyid + clint96 + aflcio97, family=binomial(link="logit"), data=impeach)

# Get out predicted values for each model
model1_pred <- predict(model1, impeach, type = "response")
model2_pred <- predict(model2, impeach, type = "response")
preds_list <- cbind(model1_pred, model2_pred)


m <- length(preds_list)
actual_values <- rep(list(impeach$votebin),2)

pred <- prediction(preds_list, actual_values)
rocs <- performance(pred, "tpr", "fpr")
#jpeg(file="ROCplot.jpeg")

# Plot it
plot(rocs, col = as.list(1:m), main = "Model 1 and 2 ROC Comparison")
     legend(x = "bottomright",
            legend = c("Model 1", "Model 2"),
            fill = 1:m)
#dev.off()

# Calculating the AUC from the predicted values calculated above
AUC_in <- performance(pred, measure="auc")

AUC_labels <- c("Model 1", "Model 2")

knitr::kable(cbind(AUC_labels, round(as.numeric(AUC_in@y.values), 3)), col.names = c("", "AUC"),
      caption = "Model AUCs")
     
```

### Out-of-sample Prediction

We assess out-of-sample prediction because our model might be overfitted to the data we feed it. When a model can predict both in-sample and out-of-sample well then it's more in line with the data generating process.

What we need to evaluate a model's prediction error (AKA generalization error, generalization performance): - Training set - Test set - A model - A loss function to measure deviation from the actual value in the test set

What we want: - Good performance in both the test and training sets

```{r, warning=FALSE}
# Load in some data
ms<-read.table("Data/Msrepl87.asc", header=TRUE, 
    colClasses=c("character",rep("numeric",22)))
rownames(ms) <- ms$country


# Create a new variables of interest
ms$sanctions <- (ms$sanctions70 + ms$sanctions75)/2
ms$deaths <- ifelse(ms$deaths75 > 0, 1, 0)


# Create 3 competing logit models
model1 <- glm(deaths ~ sanctions, family=binomial(link="logit"), data=ms)

model2 <- glm(deaths ~ sanctions + pop75 + civlib76, 
              family=binomial(link="logit"), data=ms)

model3 <- glm(deaths ~ sanctions + civlib76 + pop75 + sanctions:civlib76,
              family=binomial(link="logit"), data=ms)

modelsummary(list("Model 1"=model1, "Model 2"=model2, "Model 3"=model3))

# Based on the AIC and BIC Model 1 has the best in-sample fit
# Now let's use cross-validation to assess out-of-sample fit

set.seed(1234)

# Define number of folds we want
k <- 10

# Make the fold assignments, order them, and attach them to the dataset
folds <- cvFolds(n=NROW(ms), K=k, type="random")
folds <- data.frame(cbind(folds$subsets, folds$which))
colnames(folds) <- c("index", "fold")
folds <- folds[order(folds$index),]

ms$fold <- folds$fold

data <- ms %>%
  mutate(preds1 = rep(0, nrow(ms)),
         preds2 = rep(0, nrow(ms)),
         preds3 = rep(0, nrow(ms)))

# Loop through each fold
for(k_i in 1:k){
   
   # Get the training data
   train_data <- ms %>% filter(fold != k_i)
   
   # Get the testing data
   test_data <- ms %>% filter(fold == k_i)
   
   # Train model 1 and get predictions
   trained_model1 <- glm(deaths ~ sanctions, family=binomial(link="logit"),
                         data=train_data)
   
   preds1.vals <- predict(trained_model1, test_data, type = "response")
   
   data[data$fold == k_i,]$preds1 <- preds1.vals

   # Train model 2 and get predictions
   trained_model2 <- glm(deaths ~ sanctions + pop75 + civlib76,
                         family=binomial(link="logit"), data=ms)
   
   preds2.vals <- predict(trained_model2, test_data, type = "response")
   
   data[data$fold == k_i,]$preds2 <- preds2.vals

   # Train model 3 and get predictions  
   trained_model3 <- glm(deaths ~ sanctions + civlib76 + pop75 + sanctions:civlib76,
                         family=binomial(link="logit"), data=ms)
   
   preds3.vals <- predict(trained_model3, test_data, type = "response")
   
   data[data$fold == k_i,]$preds3 <- preds3.vals

}


preds <- list(data$preds1, data$preds2, data$preds3)
n <- length(preds)

# Get the actual values
actual_vals <- rep(list(data$deaths), n)

# Make the ROC plot
pred_instance <- prediction(preds, actual_vals)

# how to get the actual 
AUC_out <- performance(pred_instance, measure="auc")

AUC_labels <- c("Model 1", "Model 2", "Model 3")

kable(cbind(AUC_labels, round(as.numeric(AUC_out@y.values), 3)), col.names = c("", "AUC"),
      caption = "Out-of-sample AUCs")


rocs <- performance(pred_instance, "tpr", "fpr")
plot(rocs, col = as.list(1:n), main = "Out-of-sample ROC Curves from 10-fold CV")
legend(x = "bottomright", legend = c("Model 1", "Model 2", "Model 3"),
       fill = 1:n)

cvFit(model1, data=ms, y=ms$deaths, K=10)
cvFit(model2, data=ms, y=ms$deaths, K=10)
cvFit(model3, data=ms, y=ms$deaths, K=10)
```

### Separation Plots

Use this plot to assess a model's ability to discriminate between cases. The best model would have all the events (dark red lines) cluster to the right and non-events (light red lines) cluster to the left.

```{r}
# Generate predicted values for each model
model1_pred <- predict(model1, ms, type = "response")
model2_pred <- predict(model2, ms, type = "response")
model3_pred <- predict(model3, ms, type = "response")

# Note: the separationplot library doesn't seem to allow multiple plots in the same chunk or like any attempts to save the plots using R functions
separationplot(model1_pred, ms$deaths, heading="Model 1 Separation Plot")
```

```{r}
separationplot(model2_pred, ms$deaths, heading="Model 2 Separation Plot")
```

```{r}
separationplot(model3_pred, ms$deaths, heading="Model 3 Separation Plot")
```

## Ordered Categorical Variable Models

### Ordered Logit

Binary logit is just a special case of ordered logit. Because our $Y$ is discretized, the probability density function for this model is a piecewise function. The likelihood function then becomes a product of binary logit models (see pg. 143-144 of Ward and Ahlquist 2018). Like binary logit, ordered logit assumes the errors follow a logistic distribution whereas probit assumes normally distributed errors.

See pg. 146-154 of Ward and Ahlquist (2018) for examples of interpreting the confusing output an ordered logit model.

Note: The derivation of the MLE for ordered logit is sufficiently complex that I doubt it would be asked on the exam.

An example in R:

```{r, results='asis'}
drury <- read_csv("Data/drury_jpr_data.csv")

model_logit_drury <- polr(as.ordered(result) ~ log(gnprat) + trade + tarcst + cost + coop, 
                   method="logistic", data=drury, Hess=T)

modelsummary(model_logit_drury)
```

```{r}
attach(drury)
X.coop1 <- cbind(median(log(gnprat)), median(drury$trade), median(tarcst), median(cost), min(coop)) # coop = 1 scenario

X.coop4 <- cbind(median(log(gnprat)), median(drury$trade), median(tarcst), median(cost), max(coop)) # coop = 4 scenario

draws<-mvrnorm(1000, c(coef(model_logit_drury),model_logit_drury$zeta),
               solve(model_logit_drury$Hessian))

B<-draws[,1:length(coef(model_logit_drury))]

Taus<-draws[,(length(coef(model_logit_drury))+1):ncol(draws)]

pi.cutoff12.coop1 <- plogis(Taus[,2] - B%*%t(X.coop1)) - plogis(Taus[,1] - B%*%t(X.coop1))
pi.cutoff12.coop4 <- plogis(Taus[,2] - B%*%t(X.coop4)) - plogis(Taus[,1] - B%*%t(X.coop4))
pi.cutoff23.coop1 <- plogis(Taus[,3] - B%*%t(X.coop1)) - plogis(Taus[,2] - B%*%t(X.coop1))
pi.cutoff23.coop4 <- plogis(Taus[,3] - B%*%t(X.coop4)) - plogis(Taus[,2] - B%*%t(X.coop4))

# result = 2 first difference
fd.res2 <- pi.cutoff12.coop4 - pi.cutoff12.coop1
# result = 3 first difference
fd.res3 <- pi.cutoff23.coop4 - pi.cutoff23.coop1

plot(density(fd.res3, adjust=1.5), xlim=c(-0.2,0.2),ylim=c(0,25),
     xlab="Change in predicted probability", bty="n", col=1,
     yaxt="n", lwd=2, main="First Difference Change in Probability\n from Changing coop=4 to coop=1",
     ylab="")
lines(density(fd.res2, adjust=1.5), col=grey(0.5), lwd=2, lty=2)
text(x=0.1, y=20, labels="Dashed: Pr(result=2 | coop=4) -
     \n Pr(result=2 | coop=1)",cex=.8)
text(x=-0.1, y=20, labels="Solid: Pr(result=3 | coop=4) -
     \n Pr(result=3 | coop=1)",cex=.8)
```

The above plot shows the changes in probability of the `result` of sanctions being 3 and 4 when a country has `coop` = 1 (no cooperation) versus having `coop` = 4 (major trading partners making major efforts to limit trade). This change in `coop` appears to decrease the probability that a country falls into category 3 of `result` as the solid distribution above is centered to the left of 0. This makes sense given the large and negative coefficient on `coop` in the ordered logit model in the table above. The same change in `coop` may slightly increase the probability of being categorized as `result` = 2 as the dashed distribution above appears to be centered slightly to the right of 0.

### Multinomial Logit

When are DV is binned into non-ordinal categories we can use a multinomial logit model.

Again, the derivation of the likelihood function for this class of models is fairly complicated (and taking its derivative analytically would be a nightmare) and so I'd guess it's unlikely to be asked on the exam. See pg. 163-164 of Ward and Ahlquist (2018) for the relevant definitions.

#### Independence of Irrelevant Alternatives (IIA)

IIA is an assumption baked in to the standard multinomial logit model. It just means if we have some choices A and B for the DV then introducing some C won't change the relative likelihood of someone choosing A over B.

#### Interpretation

Like with ordered logit, interpretation is kind of a nightmare. Essentially, each coefficient is the logit coefficient w.r.t. the excluded (reference) category of the DV for those columns of the BUTON. See pg. 169 of Ward and Ahlquist (2018) for an example of this. The first two columns of Table 9.1 present the logit coefficient of the given category w.r.t Labor.

## Count Data

Count models are used to describe count data ($Y\in{0,1,2, . . .}$). Count data is discrete and bounded from below.

### Why not use OLS?

This is a bad idea for the same reason it's a bad idea to use OLS with binary data:

1.  The variance of a count increases with the mean (more error around larger values) and thus count data is heteroskedastic.
2.  OLS will generate predictions that aren't lower bounded at 0 or integers. This can be worked around by logging the DV but then you have to deal with $\log(0)$. Do you discard these observations? Do you add some arbitrary constant to every value of your DV?

### Poisson Regression

#### The Poisson Distribution

Assumptions:

1.  Events occur independently.
2.  There is a constant arrival rate of events, $\lambda$. The probability of an event occurring in the interval $(t,t+h]$ is then $h\lambda$ (probability of a no event is $1-\lambda h$)

The distribution is then:

$$Y\sim f_p(y;\lambda,h)$$ $$Pr(Y=y)=\frac{e^{-h\lambda}(h\lambda)^y}{y!}$$ where $\lambda>0, h>0$

This is our *stochastic/random* component. The systematic component is some continuous or discrete set of IVs, $X$, that are linear in parameters (i.e. not squared, divided, etc.).

The mean and variance of the Poisson distribution is: $E[Y]=var(Y)=h\lambda$

If all observational intervals are the same length then we can let $h=1$ and the distribution simplifies to:

$$Pr(Y=y)=\frac{e^{-\lambda}\lambda^y}{y!}$$

The mean and variance of the Poisson distribution is then: $E[Y]=var(Y)=\lambda$

#### The Poisson Model

To incorporate covariates into a Poisson model where the mean is greater than zero, we need a link function to map our covariates, $x_i$, onto positive values. We use the exponential function:

$$E[Y_i]=h\lambda_i=he^{x_i^T\beta}$$

Or when $h=1$:

$$E[Y_i]=\lambda_i=e^{x_i^T\beta}$$

Plugging this value of $\lambda$ into our probability mass function we found defined above we get:

$$Pr(Y_i=y|x_i)=\frac{e^{-e^{x_i^T\beta}}[e^{x_i^T\beta}]^y} {y!}$$

We then derive the likelihood as follows:

$$\mathcal{L}(\beta|X,y)=\prod_{i=1}^n\frac{e^{-e^{x_i^T\beta}}[e^{x_i^T \beta}]^{y_i}}{y_i!}$$

Take the log:

$$\ln\mathcal{L}=\sum_{i=1}^n [e^{x_i^T\beta} + y_ix_i^T\beta - \ln(y_i!)]$$

We can drop the $\ln(y_i!)$ as it's not a function of what we're trying to maximize ($\beta$)

The score equation is:

$$\frac{\partial\ln\mathcal{L}}{\partial\beta}=\sum_{i=1}^n y_ix_i^T\beta-x_i^T\beta e^{x_i^T\beta}=\sum_{i=1}^n(y_i-e^{x_i^T\beta})x_i=0$$

#### Differential exposure

When observations have different exposure windows (i.e. $h$ is not constant across observational units) then we would expect their counts to be proportional to their exposure window. We have two choices for accounting for this:

1.  Offset: Include the size of the exposure window in the regression but constrain its coefficient to be 1.0 i.e. $\lambda_i=e^{x_i^T\beta+\ln(h_i)}$

<!-- -->

2.  Control: Simply add the (log) of the exposure for each unit as a control in the regression

#### Interpretation

As with other non-linear models, directly interpreting coefficients isn't the way to go. Instead, construct scenarios and generate predicted values to interpret results.

An example in R:

```{r}
# Load in the data
ms<-read.table("Data/Msrepl87.asc", header=TRUE, 
    colClasses=c("character",rep("numeric",22)))
rownames(ms) <- ms$country

# Generate Poisson model
model_poisson <- glm(deaths75 ~ sanctions75 + pop75 + civlib76, data=ms,
                     family="poisson")

modelsummary(model_poisson, title="Poisson Model", stars=T)

# Now make a predicted value plot to interpret the effect of sanctions75
# on the probability of a death occurring

# Note this function automatically holds other IVs at their mean.

effect_plot(model_poisson, pred="sanctions75", interval=T) + theme_minimal()

```

#### Diagnostics

The Poisson distribution assumes that the level of heteroskedasticity will be constant. i.e. the variance will increase 1:1 with the mean of count data. This restrictive assumption is expressed as follows:

$$E[Y]=\mathrm{var}(Y)$$

We call it over-dispersion when $E[Y]<\mathrm{var}(Y)$ (most common) and under-dispersion when $E[Y]>\mathrm{var}(Y)$.

Obviously comparing the mean and variance of the DV is one way to assess dispersion. We can also use the `dispersiontest()` function from the `AER` library:

```{r}
AER::dispersiontest(model_poisson, alternative="two.sided")
```

To visually check for disperion we can use a Poissonness plot. Similar to a QQ plot, a perfectly dispersed DV will create a straight line in the plot. The Poissonness plot, however, does not tell us much about over-dispersion. For that we can use a hanging rootogram.

```{r}
# vcd has the distplot function to create a Poissonness plot but I went ahead and 
# adapted it to output a ggplot

plotness::plotness(ms$deaths75)

```

#### Dealing with over-dispersion

So we have over-dispersion, what do we do?

1.  Quasipoisson: Allows generating generating predicted counts and uncertainty estimates (which will be larger than a Poisson model) but without being able to assess model fit with information criteria or maximized likelihood.

2.  Negative binomial: A model in which $\lambda_i$ is a random variable distributed as a gamma distribution but $Y_i$, conditional on $\lambda_i$, is still Poisson

```{r}
# Generate Poisson model
model_poisson <- glm(deaths75 ~ sanctions75 + pop75 + civlib76, data=ms,
                     family="poisson")

# Generate Poisson model
model_quasipoisson <- glm(deaths75 ~ sanctions75 + pop75 + civlib76, data=ms,
                     family="quasipoisson")


# Generate negative binomial model
model_negbin <- glm.nb(deaths75 ~ sanctions75 + pop75 + civlib76, data=ms)

modelsummary(list("Poisson Model"=model_poisson, 
                  "Quasipoisson Model"=model_quasipoisson, 
                  "Neg. Bin. Model"=model_negbin), 
             title="Count Data Models", stars=T)
```

#### Dealing with under-dispersion

See Ward and Ahlquist (2018) for: Hurdle models (pg. 210) and zero-inflation models (pg. 211-212).

# 2021 Exam Questions and Answers

## 1 OLS

The attached dataset has a measure of party ideology ($Y$), a lagged measure of party ideology ($Y_\mathrm{lag}$),mean public opinion of group X ($X$), and mean public opinion of group Z ($Z$), in the variables $X$, $Y$, $Y_\mathrm{lag}$, and $Z$. The dataset is a time-series dataset organized by row, where row 1 corresponds to period 1, row 2 corresponds to period 2, etc. A scholar hypothesizes that party platforms change over time in response to current public opinion of group X and Z. Conduct your analysis below accordingly. Assume that public opinion change is exogenous.

1. Conduct a joint test of whether public opinion of group X and Z have any impact on party ideology. Explain the test you are using and conduct the test using your choice of software.

To test whether X and Z have a joint impact on Y, I conduct an F-test which compares an unrestricted OLS model containing the covariates of interest, $Y \sim X + Z$, to a restricted model wherein the mean of Y is predicted for each observation and only the intercept is estimated. In this F-test, the null hypothesis is that the coefficients on X and Z are zero and the two-tailed alternative hypothesis is that their coefficients are not zero. `R`'s `lm` function conducts a joint F-test for us when estimating the unrestricted model as follows:
```{r}
df_q1 <- read_csv("Methods_Comp_2021_Data/Question1data.csv")

model_q1.1 <- lm(Y ~ X + Z, data=df_q1)
summary(model_q1.1)
```
The joint F-statistic is estimated at 0.127 with a p-value of 0.8808. If we set our significance threshold to $\alpha=.05$ or even $\alpha=.10$ then our F-statistic is not significant and thus the null hypothesis that the coefficients on X and Z are zero cannot be rejected. We thus conclude that there is no evidence here that the public opinion of groups X or Z have an impact on party ideology.


2. For 1.1, show the formula for the test statistic and explain each element in it. Explain why OR prove that the test you are using follows the distribution that is used in whatever test you use.

The F-statistic is defined as:

$$F=\frac{(SSR_{\mathrm{restricted}}-SSR_{\mathrm{unrestricted}})/q}{SSR_{\mathrm{unrestricted}}/(n-k-1)}$$

$SSR_{\mathrm{restricted}}$ is the sum of squared residuals for our unrestricted OLS model and $SSR_{\mathrm{unrestricted}}$ is the sum of squared residuals for the restricted model. The sum of squared residuals is a measure of how much variance there is in the error terms of a regression model, essentially measuring the fit of the data to the model. By getting the difference between the $SSR$ of the two models we are getting at how much bigger the residuals are when the covariates are excluded. $n$ is the number of observations for our data. $k$ is the number of explanatory variables we have in the unrestricted model (in this case $k=2$ as we have $X$ and $Z$). $q$ is the number of restrictions we have, i.e. the difference in degrees of freedom between the unrestricted and restricted models.


The joint F-test should follow an F-distribution. We can run the following simulation to see if the F statistics we get are distributed as we would expect under the null hypothesis:
```{r}
# Note: simulation code adapted from Bert Wilden's answers to the exam
N <- 250
k <- 2
q <- 2
null_Fs <- c()

for(i in 1:1000){
  # Under the null our betas are 0
  b1_null <- 0
  b2_null <- 0
  
  # Generate normally distributed predicted values of Y as a function of X and Z 
  # assuming the null is true
  y_null <- rnorm(N, mean=0 + (b1_null * df_q1$X) + (b2_null * df_q1$Z))
  
  
  # Fit the unrestricted model predicting y_null from X and Z
  model_null_ur <- lm(y_null ~ df_q1$X + df_q1$Z)
  
  # "Fitting" the restricted model with only the intercept will just give us the
  # mean of Y so we have:
  # model_null_r <- mean(y_null)
  model_null_r <- lm(y_null ~ 1)
  
  # Get the unrestricted sum of squares
  SSR_ur <- sum(resid(model_null_ur)^2)
  
  # Get the restricted sum of squares
  SSR_r <- sum(resid(model_null_r)^2)
  
  F_stat <- ((SSR_r-SSR_ur)/q)/(SSR_ur/(N-k-1))
  
  null_Fs <- c(null_Fs, F_stat)
  
}

# Coerce into a dataframe and calculate the density under the F distribution
# for each value of the F stat simulated under the null
sim.q1 <- data.frame(null_Fs) %>% mutate(F_density = df(null_Fs, k, N-k))

ggplot(sim.q1, aes(x=null_Fs)) +
  geom_density(fill="purple3", alpha=0.5, color="black") +
  geom_line(aes(y=F_density), color="darkgreen", linetype="dashed", lwd=1) +
  theme_bw()

# OR do it in base R graphics

hist(null_Fs, br=20, col="cadetblue", freq=F, main="Simulated F Statistics\nCompared to F Fistribution")
curve(df(x,k,250-k), add=T, lwd=4, col="violetred4")
```
We can see that the simulated values under the null (purple) approximately follows an F-distribution (green dashed line) as we would expect.

3. Does one group matter more than the other? Test whether political party platforms are more responsive to Group X than to Group Z. Explain the test, state null and alternative hypothesis, and conduct the test using your favorite software. Then, show the formula used in this test and explain each component of it. You do not need to repeat any explanation already presented in a previous response.

```{r}
model_q1.1 <- lm(Y ~ X + Z, data=df_q1)
summary(model_q1.1)
```
To test whether party ideology is better predicted by $X$ or $Z$ we re-estimate the OLS model containing both covariates as in 1.1. This time we focus in on the estimated coefficient and t-value for each covariate. The t-value is calculated as:

$$t=\frac{\hat{\beta}}{\hat{SE}_{\beta}}$$

where $\hat{\beta}$ is the coefficient estimate for the covariate tested and $\hat{SE}_{\beta}$ is the coefficient's standard error. The null hypothesis is that the estimate for each covariate's coefficient, $\hat{\beta}$ is zero. The alternative hypothesis is that it is non-zero. 

We can see from the regression result that $X$ has a t-value of 0.439 and $Z$ has a t-value of 0.233. Though $X$'s estimated coefficient (0.18) is larger than $Z$'s (.05), neither t-statistic is statistically significant indicated by p-values for each coefficient $>.05$. The standard errors on the coefficients are sufficiently large that we cannot conclude that $X$ or $Z$ matter for predicting $Y$ and as such cannot conclude that the party is more responsive to one group over the other based on this model.

4. Now, suppose that opinions for Group Z are measured with error, so Z = true mean ideology + measurement error, where the measurement error is iid N(0, 1), and uncorrelated with $X$, $Y$ , $Y_\mathrm{lag}$, and $Z$ but opinions for Group X are not measured with error. How might that affect your conclusions from (3)? Develop your response with an analytic or simulated result to illustrate the problem.

```{r}
set.seed(1234)

sim.q1.4 <- data.frame(beta_X_em = NA, SE_X_em = NA, 
                       beta_Z_em = NA, SE_Z_em = NA)
# Run the simulation 10,000 times (1,000 isn't enough to see the biasing)
for(i in 1:10000){
  N <- 250
  
  # Add on normally distributed measurement error to Z
  df_q1$Z_witherror <- df_q1$Z + rnorm(n=N)
  
  # Estimate the model with measurement error
  model_Zerror <- lm(Y ~ X + Z_witherror, data=df_q1)
  
  # Pull out estimates
  beta_X_em <- summary(model_Zerror)$coefficients[2,1]
  SE_X_em <- summary(model_Zerror)$coefficients[2,2]
  
  beta_Z_em <- summary(model_Zerror)$coefficients[3,1]
  SE_Z_em <- summary(model_Zerror)$coefficients[3,2]
  
  sim.q1.4 <- rbind(sim.q1.4, c(beta_X_em, SE_X_em,
                                beta_Z_em, SE_Z_em))
}

sim.q1.4 %>% 
  drop_na() %>% 
  ggplot(aes()) +
    geom_density(aes(x=beta_X_em), fill="red", alpha=0.25, color="darkred") +
    geom_density(aes(x=beta_Z_em), fill="blue", alpha=0.25, color="darkblue") +
    geom_vline(aes(xintercept=mean(beta_X_em)), color="red", lwd=.8,
               linetype="dashed") +
    geom_vline(aes(xintercept=summary(model_q1.1)$coefficients[2,1]),
               color="darkred", linetype="dotted", lwd=1) +
    geom_vline(aes(xintercept=mean(beta_Z_em)), color="blue", lwd=.8, 
               linetype="dashed") +
    geom_vline(aes(xintercept=summary(model_q1.1)$coefficients[3,1]),
               linetype="dotted", color="darkblue", lwd=1) +
    xlim(c(-0.1,0.25)) +
    labs(x="Coefficient Estimate", y="Density", 
         title="Comparing OLS Coefficients with and without Z measurement error") +
    theme_minimal() +
    annotate("text", x=-.01, y=100, label="Z (measurement error model)", 
             color="blue", size=3) +
    annotate("text", x=.1, y=100, label="Z (no measurement error model)",
             color="darkblue", size=3) +
    annotate("text", x=0.125, y=110, label="X (measurement error model)", 
             color="red",size=3) +
    annotate("text", x=.215, y=110, label="X (no measurement\nerror model)",
             color="darkred", size=3)

sim.q1.4 %>% 
  drop_na() %>% 
  ggplot(aes()) +
    geom_density(aes(x=SE_X_em), fill="red", alpha=0.25, color="darkred") +
    #geom_density(aes(x=SE_Z_em), fill="blue", alpha=0.25, color="darkblue") +
    geom_vline(aes(xintercept=mean(SE_X_em)), color="red", lwd=.8,
               linetype="dashed") +
    geom_vline(aes(xintercept=summary(model_q1.1)$coefficients[2,2]),
               color="darkred", linetype="dotted", lwd=1) +
    # geom_vline(aes(xintercept=mean(SE_Z_em)), color="blue", lwd=.8, 
    #            linetype="dashed") +
    # geom_vline(aes(xintercept=summary(model_q1.1)$coefficients[3,2]),
    #            linetype="dotted", color="darkblue", lwd=1) +
    labs(x="Standard Error Estimate", y="Density", 
         title="Comparing OLS SEs with and without Z measurement error") +
    theme_minimal() +
    # annotate("text", x=-.01, y=100, label="Z (measurement error model)", 
    #          color="blue", size=3) +
    # annotate("text", x=.1, y=100, label="Z (no measurement error model)",
    #          color="darkblue", size=3) +
    annotate("text", x=0.3995, y=2000, label="SE_X (measurement error model)",
             color="red",size=3) +
    annotate("text", x=0.3996, y=1800, label="SE_X (no measurement error model)",
             color="darkred", size=3)


sim.q1.4 %>% 
  drop_na() %>% 
  ggplot(aes()) +
    geom_density(aes(x=SE_Z_em), fill="blue", alpha=0.25, color="darkblue") +
    geom_vline(aes(xintercept=mean(SE_Z_em)), color="blue", lwd=.8,
               linetype="dashed") +
    geom_vline(aes(xintercept=summary(model_q1.1)$coefficients[3,2]),
               linetype="dotted", color="darkblue", lwd=1) +
    xlim(c(0.12,0.23)) +
    labs(x="Standard Error Estimate", y="Density", 
         title="Comparing OLS SEs with and without Z measurement error") +
    theme_minimal() +
    annotate("text", x=0.162, y=90, label="SE_Z (measurement error model)",
             color="blue", size=3) +
    annotate("text", x=.218, y=90, label="SE_Z (no measurement error model)",
             color="darkblue", size=3)
```
The first figure above plots the distribution and mean (dashed line) of the coefficients for X and Z estimated for 10,000 different values of Z with random measurement error simulated. The dotted lines represent the coefficient estimates found in part 1.3 for the OLS model with no measurement error on Z. As the figure shows, while measurement error on Z does not affect the estimate for X's coefficient, it does bias the estimate of Z's coefficient, in this case it biases it toward zero. This means that measurement error on Z can obfuscate the effect of Z on Y.

The next two figures plot the distribution and mean (dashed line) of standard error estimates from the simulation for X and Z, respectively. The standard error estimates for the OLS model from part 1.3 are plotted with dotted lines. There is no effect of measurement error for Z on the standard error estimates for X but measurement error on Z does bias its standard error estimates toward zero.

5. Suppose that Q (included in your dataset) is a valid instrument for $Z$. Repeat your analysis from 1.3 appropriately and explain your new conclusion.

Assuming $Q$ affects $Y$ only through $Z$ we can perform an two stage least squares regression as follows:
```{r}
# Estimate the first stage regression: Z ~ Q
model_stage1 <- lm(Z ~ Q, data=df_q1)

# Grab the fitted values from the first stage regression and attach to our
# dataset
df_q1$ZonQ <- model_stage1$fitted.values

# Perform the second stage regression replacing Z with our fitted values from
# stage 1:
model_stage2 <- lm(Y ~ X + ZonQ, data=df_q1)

# Because lm doesn't calculate instrumental standard errors correctly we can
# re-estimate our IV regression with ivreg from the AER package:
model_stage2_ivreg <- ivreg(Y ~ X + Z | X + Q, data=df_q1)

# Compare these models' estimates to that of part 1.3
modelsummary(list("Non-IV Model"=model_q1.1, 
                  "IV Model (lm)"=model_stage2, 
                  "IV Model (ivreg)"=model_stage2_ivreg), stars=T)
```
We can see that using $Q$ as an instrument for $Z$ yields a coefficient that is the opposite sign and further from zero than using $Z$ itself, however, because the standard errors remain large, we cannot draw any substantively different conclusion from the IV model as opposed to the non-IV model estimated in part 1.3.

6. This is a time-series dataset. What problems may arise when conducting this type of analysis? What are some of the solutions you might use to address these problems.

The primary concern with performing OLS regression on time series is the presence of serial autocorrelation. This violates the assumption required by OLS that observations are independent of one another. This can cause errors to be correlated over time and bias the estimated standard errors. 

One way to address this is to use autoregressive (AR) or autoregressive moving average (ARMA) models rather than ordinary least squares. AR and ARMA models use previous values as predictors to account for serial autocorrelation and the resulting bias in standard errors. However, these models assume stationarity of the time series data so tests such as the ADF, KWSS, and PP statistical tests must be conducted first and differencing/other data transformation/decomposition may be necessary to meet the conditions necessary for these models.

## 2 MLE

For this question, we will be using a replication dataset from a forthcoming article in the American Journal of Political Science, Differences in Mens and Womens Policing Behavior: Evidence from Traffic Stops by Kelsey Shoub, Katelyn E. Stauffer, and Miyeon Song. Shoub, Strauffer, and Song (Forthcoming) use data from over four million traffic stops in North Carolina and Florida to understand how gender is associated with the likelihood that an officer searches someone they have stopped at a traffic stop. You can find the full paper here: https://drive.google.com/file/d/1JyNzTF3lZq8N0blVahb7Zko3M5clPcHk/view and replication data here: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/QTUF6D.

For our purposes, we will be using a smaller version of their full analysis, looking at 2,712,478 traffic stops in Florida. Note that our analyses are simpler versions of what the authors do in the paper, so this is not an exact replication, though the conclusions are the same.

Download the data (sent to you) and load it into R. We are interested in exploring how whether a search occurred `search_occur` is associated with the officers gender `of_gender`, coded 1 for female and 0 for male.

1. In this question, we will model `search_occur` as a function of `of_gender` using logistic regression. Write down the data generating process for the logit model.

```{r}
# Warning: Loading this file crashes RStudio like half the time
df_q2 <- get(load("Methods_Comp_2021_Data/Question2Data.rdata"))
```

Our stochastic component is modeled as a Bernoulli distribution:

$$
f_B(y_i;\theta_i)=\theta_i^{y_i}(1-\theta_i)^{1-y_i}= 
   \left\{
\begin{array}{ll}
      \theta_i & y_i=1 \\
      1-\theta_i & y_i=0
\end{array} 
\right. 
$$

And our systematic component is:

$$
\theta_i=\mathrm{logit}^{-1}(x_i^T\beta)=\frac{1}{1+e^{-x_i^T\beta}}
$$

2. Derive the log-likelihood function (show your work).

We first define the joint probability distribution of $i$ independent Bernoulli trials by taking the product of our stochastic component defined in part 1:

$$
Pr(y|\theta)=\prod_{i=1}^n \theta_i^{y_i}(1-\theta_i)^{1-y_i}
$$

We define this joint probability distribution as our likelihood function, $\mathcal{L}(\theta|y)$:

$$\mathcal{L}(\theta|y)=\prod_{i=1}^n \theta_i^{y_i}(1-\theta_i)^{1-y_i}$$

Taking the natural logarithm our product becomes a sum and, applying log laws, we arrive at the log likelihood:

$$\ln(\mathcal{L}(\theta|y))=\ln[\prod_{i=1}^n \theta_i^{y_i}(1-\theta_i)^{1-y_i}]$$
$$\ln(\mathcal{L}(\theta|y))=\sum_{i=1}^n\ln[ \theta_i^{y_i}(1-\theta_i)^{1-y_i}]$$
$$\ln(\mathcal{L}(\theta|y))=\sum_{i=1}^n y_i\ln(\theta_i) + (1-y_i)\ln(1-\theta_i)$$
where $\theta_i=\mathrm{logit}^{-1}(x_i^T\beta)=\frac{1}{1+e^{-x_i^T\beta}}$ as defined above.

3. Program the log-likelihood function in `R` and optimize for the coefficients on `of_gender`. Report your coefficients and standard errors in a table.

We find the MLE of the above log likelihood with `R` as follows:
```{r}
#function for optimization of logit
binreg<- function(X,y,method="BFGS"){
  X<- cbind(1,X)
  
  # Get the log-likelihood
  negLL<- function(b,X,y){
    p<-as.vector(1/(1+exp(-X %*% b)))
    
    return(-sum(y*log(p) + (1-y)*log(1-p)))
  }
  
  # Get the gradient (score function)
  gradient<- function(b,X,y){
    p <- as.vector(1/(1+exp(-X %*% b)))
    
    return(-apply(((y - p)*X), 2, sum))
  }
  
  # Pass our data, log-likelihood, and gradient to optim() 
  # which will find the maximum likelihood
  results<- optim(par=rep(0,ncol(X)), 
                  fn=negLL,
                  gr=gradient, 
                  hessian=T, 
                  method=method,
                  X=X,
                  y=y)
  
  # Pull out the stuff we want
  list(coefficients=results$par,
       stderr=sqrt(diag(solve(results$hessian))),
       var=solve(results$hessian),
       deviance=2*results$value,
       converged=results$convergence==0)
}

mlebin.fit<-binreg(X=df_q2$of_gender, y=df_q2$search_occur)

# Get our logit coefficients
round(mlebin.fit$coefficients,3)

# Get out logit standard errors
round(mlebin.fit$stderr,3)


# Double check against R's baked in estimation
fit.glm <- glm(search_occur ~ of_gender,
               family = binomial(link = "logit"), 
               data = df_q2)
summary(fit.glm, signif.stars=FALSE)

# Looks good so let's put our estimates into a nice table
kable(cbind(Term=c("Intercept", "of_gender"),
            Estimate=round(mlebin.fit$coefficients,3),
            "Std. Err."=round(mlebin.fit$stderr,3)),
      caption="Logit Model")
```


4. Simulate the expected probabilities of a search at a traffic stop given a female officer and given a male officer. Create a plot of the difference, including estimates of uncertainty. Interpret the result.

```{r}
# We want to know the difference in predicted probability
# of being pulled over when an officer is male vs. female


# Create a sequence of the range of values for the 96 vote share
gender.range <- c(0,1)

# Construct the male scenario
x.lo <- c(1, 0)
X.lo <- matrix(x.lo, nrow=length(x.lo), ncol=length(gender.range))
X.lo[2,] <- gender.range

# Construct the female scenario
x.hi <- c(1, 1)
X.hi <- matrix(x.hi, nrow=length(x.hi), ncol=length(gender.range))
X.hi[2,] <- gender.range

# Generate a multivariate normal distribution
B.tilde <- mvrnorm(1000, coef(fit.glm), vcov(fit.glm))

# Generate the predicted probabilities
s.lo <- inv.logit(B.tilde %*% X.lo)
s.hi <- inv.logit(B.tilde %*% X.hi)
s.lo <- apply(s.lo, 2, quantile, c(0.025, 0.5, 0.975))
s.hi <- apply(s.hi, 2, quantile, c(0.025, 0.5, 0.975))


# Now tidy the data and plot with ggplot
pred.prob.df <- data.frame(xrange=rep(gender.range,2), 
                           y=c(s.lo[2,],s.hi[2,]),
                           conf.low = c(s.lo[1,], s.hi[1,]),
                           conf.high = c(s.lo[3,], s.hi[3,]),
                           scenario = rep(c("Male", "Female"), 
                                          each=length(gender.range)))

ggplot(data=pred.prob.df, aes(x=xrange, y=y,
                              ymin=conf.low,
                              ymax=conf.high)) +
  geom_point(size=2.5) +
  geom_errorbar(width=0.2, size=0.8) +
  labs(x="Gender", y="Predicted Probability",
       title="Predicted Probability Plot for the\nEffect of Officer Gender of Traffic Stop Occurrence") +
  theme_minimal() +
  annotate("text", x=1, y=.0015, label="Female") +
  annotate("text", x=0, y=.0045, label="Male")
```
The model predicts that male officers are far more likely (about 5 times more likely) to conduct a traffic stop.

5. A critic of the paper says that years of service might confound the relationship between gender and searches at traffic stops. On average, female officers have fewer years of service. Include `officer_years_of_service` in your regression as a control variable. Repeat the analysis in 2.3 and report your coefficients and standard errors in a table.

```{r}
#function for optimization of logit
binreg<- function(X,y,method="BFGS"){
  X<- cbind(1,X)
  
  # Get the log-likelihood
  negLL<- function(b,X,y){
    p<-as.vector(1/(1+exp(-X %*% b)))
    
    return(-sum(y*log(p) + (1-y)*log(1-p)))
  }
  
  # Get the gradient (score function)
  gradient<- function(b,X,y){
    p <- as.vector(1/(1+exp(-X %*% b)))
    
    return(-apply(((y - p)*X), 2, sum))
  }
  
  # Pass our data, log-likelihood, and gradient to optim() 
  # which will find the maximum likelihood
  results<- optim(par=rep(0,ncol(X)), 
                  fn=negLL,
                  gr=gradient, 
                  hessian=T, 
                  method=method,
                  X=X,
                  y=y)
  
  # Pull out the stuff we want
  list(coefficients=results$par,
       stderr=sqrt(diag(solve(results$hessian))),
       var=solve(results$hessian),
       deviance=2*results$value,
       converged=results$convergence==0)
}

mlebin.fit<-binreg(X=cbind(df_q2$of_gender, df_q2$officer_years_of_service), y=df_q2$search_occur)

# Get our logit coefficients
round(mlebin.fit$coefficients,3)

# Get out logit standard errors
round(mlebin.fit$stderr,3)


# Double check against R's baked in estimation
fit.glm <- glm(search_occur ~ of_gender + officer_years_of_service,
               family = binomial(link = "logit"), 
               data = df_q2)
summary(fit.glm, signif.stars=FALSE)

# Looks good so let's put our estimates into a nice table
kable(cbind(Term=c("Intercept", "of_gender", "officer_years_of_service"),
            Estimate=round(mlebin.fit$coefficients,3),
            "Std. Err."=round(mlebin.fit$stderr,3)),
      caption="Logit Model")
```

6. For each of 1 through 40 years of service, iterating by one year at a time, estimate the probability of a search at a traffic stop for a male and female officer. Plot your point estimates and confidence intervals on a plot where the x-axis is years of service, y-axis is predicted probability and there are two sets of points and confidence intervals  one for female and one for male officers. How would you respond to the critic?

```{r}
# We want to know the difference in predicted probability of a traffic stop
# by a female vs male as a function of their years of service

# Create a sequence of the range of values for years of service
service.range <- seq(1,40,1)

# Construct the male scenario
x.lo <- c(1, 0, median(df_q2$officer_years_of_service)) # Democrat scenario
X.lo <- matrix(x.lo, nrow=length(x.lo), ncol=length(service.range))
X.lo[3,] <- service.range

# Construct the female scenario
x.hi <- c(1, 1, median(df_q2$officer_years_of_service)) # Republican scenario
X.hi <- matrix(x.hi, nrow=length(x.hi), ncol=length(service.range))
X.hi[3,] <- service.range

# Generate a multivariate normal distribution
B.tilde <- mvrnorm(1000, coef(fit.glm), vcov(fit.glm))

# Generate the predicted probabilities
s.lo <- inv.logit(B.tilde %*% X.lo)
s.hi <- inv.logit(B.tilde %*% X.hi)
s.lo <- apply(s.lo, 2, quantile, c(0.025, 0.5, 0.975))
s.hi <- apply(s.hi, 2, quantile, c(0.025, 0.5, 0.975))


# Now tidy the data and plot with ggplot
pred.prob.df <- data.frame(xrange=rep(service.range,2), 
                           y=c(s.lo[2,],s.hi[2,]),
                           conf.low = c(s.lo[1,], s.hi[1,]),
                           conf.high = c(s.lo[3,], s.hi[3,]),
                           scenario = rep(c("Male", "Female"), 
                                          each=length(service.range)))

ggplot(data=pred.prob.df, aes(x=xrange, y=y,
                              ymin=conf.low,
                              ymax=conf.high,
                              color=factor(scenario))) +
  geom_point(size=.8) +
  geom_errorbar() +
  labs(x="officer_years_of_service", y="Predicted Probability of Traffic Stop",
       title="Predicted Probability Plot", color="Officer Gender") +
  scale_fill_manual(labels=c("Male", "Female"), 
                    values=c("blue", "red")) +
  theme_minimal()
```
Years of service does not appear to be a confounder in the relationship between gender and traffic stops. First, the model estimates in part 5 indicate that, regardless of gender, officers with more years of service are less likely to conduct a search. From the above plot we can see that, regardless of the years of service, a male officer is generally more likely to conduct a search than a female. 

Further, while years of service may affect the dependent variable (and indeed it does have a significant affect according to our model in part 5), it does not intuitively make sense that years of service could somehow affect an officer's gender. We should, in fact, be concerned that gender affecting the years of service, not the other way around.

# POLI 271 2022 Midterm

## Problem 1: Linear Regression

For this problem set, we will be replicating some work from our very own political science department,Hill, Seth J., and Thad Kousser. Turning out unlikely voters? A field experiment in the top-two primary. Political Behavior 38.2 (2016): 413-432. In this experiment, Hill and Kousser sent 150,000 letters to voters before Californias 2014 primary. The authors targeted the group of Californians who had turned out in general elections, but not in primaries.

In this question, we will see if these mailers increased turnout in the 2014 primary. Use the data turnout.csv posted on the course website for this analysis.

The variables of interest are:
 `yvar`: whether or not a person turned out to vote in the primary
 `mailer`: whether or not they received any mailer
 `treatment.assign`: which mailer (if any) they received (see article for a description of the different mailer treatments). This variable is "Control" if they did not receive one.
 `Party`: Party of the person who received the mailer 

Use a linear regression model to estimate the effect of treatment assignment (treatment.assign) on turnout (yvar). To do this, you should create an indicator variable for each unique value of treatment assignment.

```{r}
# Load in the data and create our indicator variables
df_q1 <- read_csv("MLE_Midterm_Data/turnout.csv") %>% 
  mutate(control=case_when(treatment.assign == "Control" ~ 1,
                           T ~ 0),
         treatment.top2=case_when(treatment.assign == "Top-two info" ~ 1,
                        T ~ 0),
         treatment.partisan=case_when(treatment.assign == "Partisan" ~ 1,
                        T ~ 0),
         treatment.election=case_when(treatment.assign == "Election info" ~ 1,
                        T ~ 0)) 
    # %>% sample_n(size=1000) # get a random subsamples so it doesn't take forever to
    #                     # run optim() later
```

1. Write out the stochastic and systematic components for this model.

Stochastic component: $Y\sim N(\theta;\sigma^2)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(y-\mu)^2}{2\sigma^2}}$

Systematic component: A linear combination of $X$ (i.e. $\beta_0 + \beta_1 x_1 + \beta_2 x_2 + . . .$) where $X\in\mathbb{R}$, expressed as $\mu=\beta X$.

2. Derive the log-likelihood.

Our probability function is: 

$$\mathrm{Pr}(Y=y)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(y_i-\mu_i)^2}{2\sigma^2}}$$
We first construct the joint probability function by taking the product of the above for $n$ independent observations:

$$f_{\mathrm{joint}}(y_i|\mu;\sigma^2)=\prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(y_i-\mu_i)^2}{2\sigma^2}}$$

We define this as our likelihood function:
$$\mathcal{L}=\prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(y_i-\mu_i)^2}{2\sigma^2}}$$
We then take the natural log of both sides and simplify by applying log laws:

$$\ln\mathcal{L}=\ln[(\frac{1}{\sigma\sqrt{2\pi}})^n\prod_{i=1}^n e^{-\frac{(y_i-\mu_i)^2}{2\sigma^2}}]$$
$$\ln\mathcal{L}=\ln[\prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(y_i-\mu_i)^2}{2\sigma^2}}]$$
$$\ln\mathcal{L}=\sum_{i=1}^n \ln[\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(y_i-\mu_i)^2}{2\sigma^2}}]$$
$$\ln\mathcal{L}=\sum_{i=1}^n[-\frac{1}{2}\ln(2\pi)-\frac{1}{2}\ln(\sigma^2)-\frac{1}{2\sigma^2}(y_i-\mu_i)^2]$$

3. Optimize the model to find the maximum likelihood estimates for  and 

```{r, warning=FALSE}
# Build our x
x <- as.matrix(cbind(1, 
           df_q1 %>% select(treatment.top2, 
                            treatment.partisan, 
                            treatment.election)))
# Build our y
y <- df_q1$yvar

# Define a function that returns the (negative) log likelihood
negLL.OLS <- function(theta,y,x){    
   n <- nrow(x)    
   k <- ncol(x)   
   beta <- theta[1:k]    
   sigma2 <- theta[k+1]    
   e <- y-x%*%beta    
   LL<- -0.5*n*log(2*pi)-0.5*n*log(sigma2)-((t(e)%*%e)/(2*sigma2))    
   return(-LL)    
}

# Optimize the likelihood function and return our estimates
result <- optim(par=rep(1,ncol(x)+1), fn=OLS, method="BFGS", hessian=TRUE, y=y, x=x)

coefficients <- round(result$par[1:ncol(x)],3)
stderrs <- round(sqrt(diag(solve(result$hessian)))[1:ncol(x)],3)

# Check against lm
model_q1 <- lm(yvar ~ treatment.top2 + treatment.partisan + treatment.election, data = df_q1)
summary(model_q1)
```

2. Calculate the standard errors for your $\hat{\beta}$s and present your results in a nicely formatted table.

```{r}
kable(cbind(Term=c("Intercept", "Top-two", "Partisan", "Election"),
      Estimate=coefficients, "Std. Err."=stderrs))
```

4. Generate a histogram of 10,000 predicted values for the number of people to turnout under the Election treatment. Plot a histogram of these predicted values.

```{r}
# To generate predicted values, because when treatment.election = 1 then all other
# covariates are zero, we have:
pred.vals <- rnorm(n=10000, mean=coefficients[1], sd=stderrs[1]) + # Intercept
             rnorm(n=10000, mean=coefficients[4], sd=stderrs[4]) # Election treatment

hist(pred.vals)
```

5. Why dont the predicted values make much sense in the context of the substantive application?

Our outcome variable is a binary variable; whether someone turned out or not. The histogram shows that the model is predicted values not within $[0,1]$. A negative predicted value makes no sense given what we're modeling. A logit model would be better suited for our data.

## Problem 2: Logit

Instead of a normal OLS model as in the last question, use a logit model to model `yvar` as a function of `treatment.assign`.

1. Write out the stochastic and systematic components for this model.

We model binary outcomes as Bernoulli trials so our stochastic component is:

$$
f_B(y_i;\theta_i)=\theta_i^{y_i}(1-\theta_i)^{1-y_i}= 
   \left\{
\begin{array}{ll}
      \theta_i & y_i=1 \\
      1-\theta_i & y_i=0
\end{array} 
\right. 
$$

And our systematic component is:

$$
\theta_i=\mathrm{logit}^{-1}(x_i^T\beta)=\frac{1}{1+e^{-x_i^T\beta}}
$$

2. Derive the log-likelihood.

We take the product of the stochastic component to get our joint probability distribution:

$$
Pr(y|\theta)=\prod_{i=1}^n \theta_i^{y_i}(1-\theta_i)^{1-y_i}
$$

Which we take the log of to get our log-likelihood:

$$
\ln\mathcal{L}(\theta|y)=\sum_{i=1}^n\ln(\frac{e^{-x_i^T\beta^{1-y_i}}}{1+e^{-x_i^T\beta}})
$$


3. Optimize the model to find the maximum likelihood estimates for . Calculate the standard errors for your s and present your results in a nicely formatted table.


```{r}
#function for optimization of logit
binreg<- function(X,y,method="BFGS"){
  X<- cbind(1,X)
  
  # Get the log-likelihood
  negLL<- function(b,X,y){
    p<-as.vector(1/(1+exp(-X %*% b)))
    
    return(-sum(y*log(p) + (1-y)*log(1-p)))
  }
  
  # Get the gradient (score function)
  gradient<- function(b,X,y){
    p <- as.vector(1/(1+exp(-X %*% b)))
    
    return(-apply(((y - p)*X), 2, sum))
  }
  
  # Pass our data, log-likelihood, and gradient to optim() 
  # which will find the maximum likelihood
  results<- optim(par=rep(1,ncol(X)), 
                  fn=negLL,
                  gr=gradient, 
                  hessian=T, 
                  method=method,
                  X=X,
                  y=y)
  
  # Pull out the stuff we want and return it
  list(coefficients=results$par,
       stderr=sqrt(diag(solve(results$hessian))),
       var=solve(results$hessian),
       deviance=2*results$value,
       converged=results$convergence==0)
}

# Build our x
x <- as.matrix(df_q1 %>% select(treatment.top2, 
                            treatment.partisan, 
                            treatment.election))
# Build our y
y <- df_q1$yvar

# Plug in our x and y created earlier
mlebin.fit<-binreg(X=x, y=y)

# Get our logit coefficients
coefficients <- round(mlebin.fit$coefficients,3)
stderrs <- round(mlebin.fit$stderr,3)

kable(cbind(Term=c("Intercept", "Top-two", "Partisan", "Election"),
      Estimate=coefficients, "Std. Err."=stderrs))

# Compare to R's baked in logit estimation
fit.glm <- glm(yvar ~ treatment.top2 + treatment.partisan + treatment.election,
               family = binomial(link = "logit"), 
               data = df_q1)
summary(fit.glm)
```

4. Generate a histogram of 10,000 *expected* values for the probability of turning out under the Election treatment. Do the same for the control treatment. Plot the two distributions of probabilities next to one another. Why is the uncertainty different in each of the histograms? On average, how much did the treatment increase the probability of turnout?

```{r}
Y.election <- rnorm(n=10000, mean=coefficients[1], sd=stderrs[1]) + 
              rnorm(n=10000, mean=coefficients[4], sd=stderrs[4])

P.election <- inv.logit(Y.election)

Y.control <- rnorm(n=10000, mean=coefficients[1], sd=stderrs[1])

P.control <- inv.logit(Y.control)

df_q2.4 <- data.frame(p=c(P.election, P.control),
                      mu=rep(c(mean(P.election), mean(P.control)), each=10000),
                      Treatment=rep(c("Election Info", "Control"), each=10000))

ggplot(data=df_q2.4, aes(x=p, fill=Treatment)) +
  geom_histogram(color="darkgray") +
  geom_vline(aes(xintercept=mu), linetype="dashed", size=1) +
  theme_minimal() +
  facet_wrap(~Treatment, nrow=2)
```
Being treated with the election info mailer, on average, increased the probability of turning out by 12.3% as compared to the control group. The uncertainty is larger on the election info probability distribution for two reasons: 1. we have more observations for the control group 2. 2. we are essentially adding the variance of the intercept and election info estimates together when simulating probabilities for the latter. When we simulate the control group we only have the variance on the intercept to consider.

5. In a target population of 3.7 million voters, approximately how many more voters would the Election treatment have gotten to vote than the control?

```{r}
cat("Election treatment would get:", round(unique(df_q2.4$mu)[[1]]*3.7e6), "voters\n")
cat("Control would get:", round(unique(df_q2.4$mu)[[2]]*3.7e6), "votSers\n")
cat("So a difference of:", 
    round(unique(df_q2.4$mu)[[1]]*3.7e6) - round(unique(df_q2.4$mu)[[2]]*3.7e6), 
    "voters")
```


## Problem 3: Poisson

In this problem, we will reproduce a simplified version of the analysis from Levin, Jamie, et al. A test of the democratic peacekeeping hypothesis: Coups, democracy, and foreign military deployments. *Journal of Peace Research* 58.3 (2021): 355-367. Levin et al hypothesize that sending peacekeepers abroad can undermine authoritarian regimes at home by enriching the military and therefore creating incentives for coups. The authors use a Poisson regression to look at the relationship between peacekeepers sent abroad and the number of coups at home. The authors include a lot of components in the model  here we will simply look at whether the number of military coup attemps `milcoupsum` is predicted by the number of peacekeeping troops sent abroad `troops` using a Poisson regression, controlling for `conflict`.

1. Write out the stochastic and systematic components for the model.

Stochastic component:
$$Y\sim f_p(y;\lambda,h)$$ $$Pr(Y=y)=\frac{e^{-h\lambda}(h\lambda)^y}{y!}$$ where $\lambda>0, h>0$

Systematic component: A linear combination of $X$, $\beta X$


2. Derive the log-likelihood.

To incorporate covariates into a Poisson model where the mean is greater than zero, we need a link function to map our covariates, $x_i$, onto positive values. We use the exponential function:

$$E[Y_i]=h\lambda_i=he^{x_i^T\beta}$$

Or when $h=1$:

$$E[Y_i]=\lambda_i=e^{x_i^T\beta}$$

Plugging this value of $\lambda$ into our probability mass function we found defined above we get:

$$Pr(Y_i=y|x_i)=\frac{e^{-e^{x_i^T\beta}}[e^{x_i^T\beta}]^y} {y!}$$

We then derive the likelihood as follows:

$$\mathcal{L}(\beta|X,y)=\prod_{i=1}^n\frac{e^{-e^{x_i^T\beta}}[e^{x_i^T \beta}]^{y_i}}{y_i!}$$

Take the log:

$$\ln\mathcal{L}=\sum_{i=1}^n [e^{x_i^T\beta} + y_ix_i^T\beta - \ln(y_i!)]$$

3. Load the data (`replication_data.dta`) using the `haven` package. Subset the data to only autocracies (`robust_autocracy6`==1). Select only the troops, conflict, and milcoupsum columns of the data. Remove any observations with `NA`s in either of these columns.

```{r}
df_q3 <- read_dta("MLE_Midterm_Data/replication_data.dta") %>% 
  filter(robust_autocracy6==1) %>% 
  select(troops, conflict, milcoupsum) %>% 
  drop_na()
```

4. Optimize the model to find the maximum likelihood estimates for . Calculate the standard errors for your $\hat{\beta}$s and present your results in a nicely formatted table.

```{r}
X <- cbind(1,df_q3 %>% select(troops, conflict))
y <- df_q3$milcoupsum

negLL.poisson <- function(par, y, X){
  X <- as.matrix(X)
  beta <- par[1:ncol(X)]

  return(-sum((X%*%beta)*y - exp(X%*%beta)))
}

fit.poisson <- optim(par = rep(0, ncol(X)), fn = negLL.poisson, y = y,
                X = X, method = "BFGS", hessian = TRUE)

coefficients <- round(fit.poisson$par,3)
stderrs <- round(sqrt(diag(solve(fit.poisson$hessian))),3)



kable(cbind(Term=c("Intercept", "Troops", "Conflict"),
      Estimate=coefficients, "Std. Err."=stderrs))

# Compare to R's baked in logit estimation
fit.glm <- glm(milcoupsum ~ troops + conflict,
               family = "poisson", 
               data = df_q3)
summary(fit.glm)
```

5. Generate a histogram of 10000 predicted values for coups when `troops`=5 and `conflict`=1. Plot this histogram next to 10000 predicted values for coups when `troops`=0 and `conflict`=1. About how many more coups would you expect with `troops`=5 than `troops`=0?

```{r}
n <- 10000
scen1 <- exp(rnorm(n, mean=coefficients[1], sd=stderrs[1]) + 
             5*rnorm(n, mean=coefficients[2], sd=stderrs[2]) +
             1*rnorm(n, mean=coefficients[3], sd=stderrs[3]))

scen2 <- exp(rnorm(n, mean=coefficients[1], sd=stderrs[1]) + 0 +
             1*rnorm(n, mean=coefficients[3], sd=stderrs[3]))

df_q2.5 <- data.frame("Predicted milsumcoup Count"=c(scen1, scen2), 
                      "Scenario"=rep(c("troops=5; conflict=1", 
                                       "troops=0; conflict=1"),each=n))

ggplot(data=df_q2.5, aes(x=Predicted.milsumcoup.Count,fill=Scenario)) +
  geom_density(color="darkgray", alpha=0.5, position="identity") +
  xlim(c(0,3)) +
  theme_minimal()

df_scen1 <- data.frame(troops = rep(5, 10000), conflict=rep(1, 10000))
df_scen2 <- data.frame(troops = rep(0, 10000), conflict=rep(1, 10000))

df_scen1$milcoupsum <- exp(predict.glm(fit.glm, df_scen1))
df_scen2$milcoupsum <- exp(predict.glm(fit.glm, df_scen2))

df_scen1$scenario <- "troops=5; conflict=1"
df_scen2$scenario <- "troops=0; conflict=0"

df_q2.5 <- rbind(df_scen1, df_scen2)

ggplot(data=df_q2.5, aes(x=milcoupsum,fill=scenario)) +
  geom_density(color="darkgray", alpha=0.5, position="identity") +
  xlim(c(0,1)) +
  theme_minimal()

```

6. Use the `AER` package to test for overdispersion. What do you conclude?

```{r}
AER::dispersiontest(fit.glm, alternative="greater")
```

The test indicates there is likely overdispersion in the data. This may be causing our standard errors to be biased toward zero and indicates a quasipoisson or negative binomial model may be a better choice for estimating the relationship between the variables.
