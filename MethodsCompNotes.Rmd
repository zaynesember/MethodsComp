---
title: "Methods Comp Notes"
author: "Zayne Sember"
date: "3/29/2022"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(sjPlot)
library(tidyverse)
library(fixest)
library(modelsummary)
library(ggeffects)
```

# POLI 204B

## Hypothesis Testing

1.  Set some threshold, $\alpha$, at or below which you deem a p-value
    significant

2.  Define the null hypothesis, $H_0$

3.  Define the alternative hypothesis, $H_A$, either two-tailed or
    one-tailed

4.  Calculate a test statistic such as Z-score or t-test and check the
    p-value

5.  Draw conclusions

Some hypothesis tests in R

```{r}
# ONE SAMPLE T-TEST
# How likely is it that the mean of some population from which we take a normal sample, x, is greater than some number, mu?

x <- rnorm(100)
t.test(x, mu=5)

# WELCH TWO-SAMPLE T-TEST
# How likely is it that the means of two populations differ based on two normal samples of equal variance, x and y?

x <- rnorm(100)
y <- rnorm(100)

t.test(x, y)

# TWO PROPORTION Z-TEST
# Is there a significant difference in proportions between two populations given normal samples of them, x and y?

x <- rnorm(100)
y <- rnorm(100)

# Assuming we want to test whether the share of values greater than zero differs between the two populations
x_count <- length(x[x>0])
y_count <- length(y[y>0])

prop.test(x=c(x_count,y_count), n=c(length(x), length(y)), alternative="two.sided")


# CHI-SQUARED TEST
# Are two variables independent? Useful for testing relationships between categorical variables

#  For example, if you were investigating the relationship between occupation and party preference, and 35% of voters were Democrats, 32% were Independents, and 33% were Republicans, independence implies the same partisan breakdown across all occupational categories (35, 32, and 33).

data_frame <- read.csv("https://goo.gl/j6lRXD")

table(data_frame$treatment, data_frame$improvement)

chisq.test(data_frame$treatment, data_frame$improvement, correct=FALSE)


# DIFFERENCE BETWEEN VARIANCES TEST
# Is the variance between two normal samples, x and y, drawn from populations significantly different?

x <- rnorm(100, sd=3)
y <- rnorm(121, sd=3.5)

var.test(x, y)
```

## Properties of Estimators

### Unbiasedness

$E(\hat{\theta}) - \theta = 0$

### Asymptotic Unbiasedness

As $n$ gets bigger, the estimator's biasedness goes to zero
$\lim_{n\to\infty} P(|E[\hat{\theta}] - \theta| > \epsilon) \to 0; \forall\epsilon >0$

### Efficiency

The estimator needs fewer observations to achieve better error
performance $\frac{1}{MSE}=\frac{1}{E[(\hat{\theta}-\theta)^2]}$

### Consistency

The distribution of estimates converges on the true value as $n$ gets
bigger
$\lim_{n\to\infty} P(|\hat{\theta} - \theta| > \epsilon) \to 0; \forall\epsilon >0$

## Correlation

We define the correlation, $r$, between two variables, $x$ and $y$, to
be:

$$r=\frac{\sum{Z_{xi}Z_{yi}}}{n-1}=\frac{\sum{(x_i-\bar{x})(y_i-\bar{y})}}{s_xs_y(n-1)}$$
where the standard deviation of $x$ (and same for $y$) is:

$$s_x=\sqrt{\frac{\sum{(x_i-\bar{x})^2}}{n-1}}$$

```{r}
x <- rnorm(n=100, mean=2, sd=3)
y <- rnorm(n=100, mean=-3, sd=2)

cor(x,y)
```

## Regression Basics

Ordinary least squares regression seeks to minimize the the vertical
distances, $\epsilon$, between the data being fitted and some line of
best fit with some slope (in the bivariate case), $\beta_1$, and some
intercept, $\beta_0$.

The model we want to fit to:

$$y_i=\beta_0+\beta_1x_i+\epsilon_i$$

The model we estimate:

$$y_i=\hat{\beta_0}+\hat{\beta_1}x_i+e_i$$ The residuals, $e_i$, are an
estimate of the error (distance between true line and y_i).

The residual sum of squares (RSS) is defined as:
$$RSS=\sum e_i^2=\sum(y_i-\hat{y_i})^2$$

```{r}
# If we have some lm model we can get the RSS as follows:
x <- rnorm(n=100, mean=2, sd=3)
y <- rnorm(n=100, mean=-3, sd=2)

model <- lm(y ~ x)

deviance(model)

# or

sum(resid(model)^2)

# Or without an lm model:

rss <- function(y, y_hat){
  return(sum((y-y_hat)^2))
}

y_hat <- fitted(model)

rss(y, y_hat)
```

The regression sum of squares (RSS) (AKA sum of squares of regression
(SSR), explained sum of squares (ESS)) is defined as:
$$RegSS=\sum(\hat{y_i}-\bar{y})^2$$

```{r}
x <- rnorm(n=100, mean=2, sd=3)
y <- rnorm(n=100, mean=-3, sd=2)

model <- lm(y ~ x)

y_hat <- fitted(model)

regSS <- function(y_hat, y){
  return(sum((y_hat-mean(y))^2))
}

regSS(y_hat, y)
```

The total sum of squares (TSS) is defined as:
$$TSS=\sum(y_i-\bar{y})^2$$

```{r}
y <- rnorm(n=100, mean=-3, sd=2)

TSS <- function(y){
  return(sum((y-mean(y))^2))
}

TSS(y)
```

The ratio of RegSS to TSS is our $r^2$, the share of variance in $y$
that is explained by $x$:
$$r^2=\frac{RegSS}{TSS}=\frac{\sum(\hat{y_i}-\bar{y})^2}{\sum(y_i-\bar{y})^2}$$

```{r}
x <- rnorm(n=100, mean=2, sd=3)
y <- rnorm(n=100, mean=-3, sd=2)

model <- lm(y ~ x)

summary(model)$r.squared

# or

y_hat <- fitted(model)

rsquared <- function(y, y_hat){
  return(sum((y_hat-mean(y))^2)/sum((y-mean(y))^2))
}

rsquared(y, y_hat)
```
### Calculating bivariate regression by hand (linear method)

#### Step 1

Given the vectors $X$ and $Y$, find $X^2$, $XY$, and the sums of all of
the preceding vectors

```{r}
calc_x2xy <- function(x, y){
  df <- data.frame(x=x,y=y)
  
  df$xsquared <- x^2
  df$xy <- x*y
  
  totals <- c(sum(x), sum(y), sum(df$xsquared), sum(df$xy))
  
  df <- rbind(df, totals)
  rownames(df) <- c(1:length(x),"total")
  
  return(df)
}

x <- rnorm(n=7, mean=2, sd=3)
y <- rnorm(n=7, mean=-3, sd=2)

df_step1 <- calc_x2xy(x=x,y=y)
df_step1
```

#### Step 2

Calculate $\hat\beta_1$ and $\hat\beta_0$:
$$\hat\beta_1=\frac{N\sum x_iy_i-\sum x_i\sum y_i}{N\sum x_i^2-(\sum x_i)^2}$$
$$\hat\beta_0=\frac{\sum y_i -\hat\beta_1\sum x_i}{N}$$

```{r}
calc_beta1 <- function(df){
  totals <- tail(df,n=1)
  N <- length(df[[1]])-1
  
  df <- head(df,n=N)
  N <- length(df$x-1) # -1 bc row totals amended
  
  beta1 <- (N*totals$xy - (totals$x*totals$y))/((N*totals$xsquared)-(totals$x)^2)
  return(beta1)
}

calc_beta0 <- function(df, beta1){
  totals <- tail(df,n=1)
  N <- length(df[[1]])-1
  
  beta0 <- (totals$y - (beta1*totals$x))/N
  return(beta0)
}

beta1 <- calc_beta1(df=df_step1)
beta1

beta0 <- calc_beta0(df=df_step1, beta1=beta1)
beta0

# check with:
# lm(y~x)
```

#### Step 3

Next, calculate $x-\bar x$, $(x- \bar x)^2$, $y-\bar y$, $(y-\bar y)^2$,
and $(x-\bar x)(y-\bar y)$.

```{r}
calc_r_table <- function(x, y){
  N <- length(x)
  df <- data.frame(x=x,y=y)
  
  df$x_minus_xbar <- df$x -  mean(x)
  df$x_minus_xbar_sq <- (df$x_minus_xbar)^2
  
  df$y_minus_ybar <- df$y -  mean(y)
  df$y_minus_ybar_sq <- (df$y_minus_ybar)^2
  
  df$x_minus_xbar_y_minus_ybar <- df$x_minus_xbar*df$y_minus_ybar
  
  return(df)
}

calc_rsquared <- function(x, y){
  df <- calc_r_table(x,y)
  N <- length(x)
  
  sd_x <- sqrt((sum(df$x_minus_xbar_sq))/(N-1))
  sd_y <- sqrt((sum(df$y_minus_ybar_sq))/(N-1))
  
  r <- (sum(df$x_minus_xbar_y_minus_ybar))/((N-1)*sd_x*sd_y)
  
  return(r^2)
}

calc_r_table(x=x,y=y)

calc_rsquared(x=x,y=y)

# check with:
#summary(lm(y~x))$r.squared
```

#### Step 4

Calculate the residuals, their square, and from that the estimated
standard error $$e_i=y_i-\hat y_i=y_i-\hat\beta_0-\hat\beta_1 x_i$$
$$\hat\sigma=\sqrt{\frac{\sum e_i^2}{N-2}}$$

```{r}
calc_residuals <- function(x, y, beta0, beta1){
  df <- data.frame(e=y-beta0-(beta1*x))
  df$e_squared <- df$e^2
  
  return(df)
}

calc_sigma <- function(e_squared, N){
  return(sqrt(sum(e_squared)/(N-2)))
}

residuals <- calc_residuals(x, y, beta0, beta1)
residuals

# check with:
#resid(lm(y~x))

sigma <- calc_sigma(residuals$e_squared, N=length(x))
sigma

# check with:
#summary(lm(y~x))$sigma


```

### Calculating bivariate regression by hand (matrix method)

Just calculate $\hat\beta=(X'X)^{-1}X'Y$

```{r}
x <- rnorm(n=7, mean=2, sd=3)
y <- rnorm(n=7, mean=-3, sd=2)

print("Append ones to X")
X <- matrix(c(rep(1, length(x)),x), nrow=length(x))
X

print("Define Y")
Y <- as.matrix(y)
Y

print("Get X'X")
t(X) %*% X

print("Get the inversion of X'X")
solve(t(X) %*% X)

print("Get X'Y")
t(X) %*% Y

print("Get beta")
solve(t(X) %*% X) %*% t(X) %*% Y

# check with:
#lm(y~x)
```

## Multiple Regression

We now have multiple $X$s we need to fit to:

$$Y=\beta_0+\beta_1X_1+\beta_2X_2+\epsilon$$

We want to find the values of $\beta_0$, $\beta_1$, and $\beta_2$ that
**minimizes** the sum of squared residuals (RSS):

$$RSS=\sum(\hat{y_i}-\bar{y})^2=\sum(\hat{y_i}-(\hat{\beta_0}+\hat\beta_1X_1+\hat\beta_2X_2)^2$$

## Categorical Variables

When regressing on a categorical variable you leave one category out. Coefficients are the impact of being $X$ vs. the excluded category, all else equal.

## Fixed Effects

Suppose we have data on 100 individuals in each of 100 countries ($n=10,000$).

If we include a dummy to "soak up" the country effects we can "isolate" the differences between the individuals across countries.
- The estimated coefficients for each country typically aren't reported bc they aren't relevant to our RQ

```{r}
data(trade)

# Without fixed effects
ols <- lm(log(Euros) ~ log(dist_km), data=trade)

summary(ols)

# With fixed effects by country of origin and destination, product, and year
fe_ols <- feols(log(Euros) ~ log(dist_km) | Origin + Destination + Product + Year, trade)

summary(fe_ols)
```

## Interactions

Case 1: Interacting a dummy with another dummy
- Coefficient is the (estimated) change in the *intercept* of the line of best fit between units where both dummies are zero vs. where they are both 1.

Case 2: Interacting a dummy with a continuous variable
- Coefficient is the difference in coefficient on the continuous variable when dummy is on vs. off.

Case 3: Interacting a continuous variable with another continuous variable.
- Don't try to interpret coefficient, look at an interaction plot

Overall, an interaction tells us the *impact* one variable has on another variable's effect on the outcome.
- Only include them when you have a theoretical reason for doing so

### Interaction plots
```{r}
set.seed(10)

#create data frame
df <- data.frame(gender = c(0,0,0,1,0,1,1,0,1,0),
                 exercise = c(0,0,2,1,2,0,1,0,2,1),
                 weight_loss = rnorm(n=10,mean=2,sd=3)) %>% 
  mutate(motivation = (2.5*weight_loss + rnorm(n=10, mean=3, sd=3)))
  
head(df)

model <- lm(weight_loss ~ gender*exercise, data=df)
summary(model)

# Plot interaction of gender*exercise
ggplot(data=df, aes(x=exercise, y=weight_loss, color=as.factor(gender))) +
  geom_point() +
  geom_smooth(method="lm") +
  labs(title="Interaction between gender and exercise") +
  theme_light()


model <- lm(weight_loss ~ exercise*motivation, data=df)
summary(model)

# To visualize an interaction between continuous variables you have to discretize
# one of them, i.e. group those above and below the mean
df$motivation_groups <- case_when(
  df$motivation > mean(df$motivation)+sd(df$motivation) ~ "high",
  df$motivation < mean(df$motivation)+sd(df$motivation) & df$motivation > mean(df$motivation)-sd(df$motivation) ~ "medium",
  df$motivation < mean(df$motivation)-sd(df$motivation) ~ "low"
)

# Can plot the interactions manually

ggplot(data=df, aes(x=exercise, y=weight_loss, color=as.factor(motivation_groups))) +
  geom_point() +
  geom_smooth(method="lm") +
  labs(title="Interaction between motivation and exercise") +
  theme_light()

# OR with plot_model
plot_model(model, type = "pred", terms = c("exercise", "motivation")) +
  theme_light()

plot_model(model, type = "int") +
  theme_light()
```

## Regression Presentation

### Regression tables

Pretty self-explanatory

### Coefficient Plots

Easier to read than a Big Ugly Table of Numbers (BUTON)
- Always include 95% CI

```{r}
# Source: https://bookdown.org/paul/applied-data-visualization/graph-coefficient-plots.html

fit <- lm(Fertility ~ Catholic + Agriculture + Education, data = swiss) # see ?swiss
results <- tidy(fit)
fit_cis_95 <- confint(fit, level = 0.95) %>% 
  data.frame() %>%
  rename("conf.low_95" = "X2.5..",
         "conf.high_95" = "X97.5..")
fit_cis_90 <- confint(fit, level = 0.90) %>% 
  data.frame() %>%
  rename("conf.low_90" = "X5..",
         "conf.high_90" = "X95..")
results <- bind_cols(results, 
                     fit_cis_95, 
                     fit_cis_90) %>%
           rename(Variable = term,
                  Coefficient = estimate,
                  SE = std.error) %>%
           filter(Variable != "(Intercept)")


ggplot(results, aes(x = Variable, y = Coefficient)) +
        geom_hline(yintercept = 0, colour = gray(1/2), lty = 2) +
        geom_point(aes(x = Variable, 
                    y = Coefficient)) + 
        # geom_linerange(aes(x = Variable, 
        #              ymin = conf.low_90,
        #              ymax = conf.high_90),
        #            lwd = 1) +
        geom_linerange(aes(x = Variable, 
                     ymin = conf.low_95,
                     ymax = conf.high_95),
                   lwd = 1/2) + 
        ggtitle("Outcome: Fertility") +
        coord_flip() +
  theme_light()
```
### Predicted Value Plots

These plots convey the substantive importance of your findings; the magnitude of the impact of your IVs on your DV

```{r}
df <- data.frame(x1=c(3, 4, 4, 5, 5, 6, 7, 8, 11, 12),
                 x2=c(6, 6, 7, 7, 8, 9, 11, 13, 14, 14),
                 x3=c(15, 12, 8, 9, 6, 5, 5, 2, 1, 0),
                 y=c(22, 24, 24, 25, 25, 27, 29, 31, 32, 36))

#fit multiple linear regression model
model <- lm(y ~ x1 + x2 +x3, data=df)

#plot predicted vs. actual values
ggplot(df, aes(x=predict(model), y=y)) + 
  geom_point() +
  geom_abline(intercept=0, slope=1) +
  labs(x='Predicted Values', y='Actual Values', 
       title='Predicted vs. Actual Values') +
  theme_light()
```
### Marginal Effects Plot

Another way to visualize the substantive meaning of your regression coefficients, especially when interactions are involved.

```{r}
data(efc)
model <- lm(barthtot ~ c12hour + neg_c_7 * c161sex + e42dep, data = efc)

ggpredict(model, terms = "c12hour")

df_predict <- ggpredict(model, terms = "c12hour")
ggplot(df_predict, aes(x, predicted)) +
  geom_line() +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .1)

# OR use plot_model
plot_model(model, type = "pred", terms = c("c12hour")) +
  theme_light()
```
## Regression Assumptions
### Four assumptions of regression

1.  *Linearity*: The relationship between $X$ and the mean of $Y$ is
    linear

2.  *Homoscedasticity*: The variance of the residual is the same for any
    value of $X$

3.  *Independence*: Observations are independent of each other

4.  *Normality*: For any fixed value of $X$, $Y$ is normally distributed
## Regression Diagnostics

# POLI 271

## Likelihood Theory

## Binary Data

## Out-of-sample Prediction

## Model Selection

## Model Interpretation

# 2021 Exam Questions and Answers
