---
title: "Methods Comp Notes"
author: "Zayne Sember"
date: "3/29/2022"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(olsrr)
library(car)
library(sjPlot)
library(tidyverse)
library(fixest)
library(modelsummary)
library(ggeffects)
```

# POLI 204B

## Hypothesis Testing

1.  Set some threshold, $\alpha$, at or below which you deem a p-value
    significant

2.  Define the null hypothesis, $H_0$

3.  Define the alternative hypothesis, $H_A$, either two-tailed or
    one-tailed

4.  Calculate a test statistic such as Z-score or t-test and check the
    p-value

5.  Draw conclusions

Some hypothesis tests in R

```{r}
# ONE SAMPLE T-TEST
# How likely is it that the mean of some population from which we take a normal sample, x, is greater than some number, mu?

x <- rnorm(100)
t.test(x, mu=5)

# WELCH TWO-SAMPLE T-TEST
# How likely is it that the means of two populations differ based on two normal samples of equal variance, x and y?

x <- rnorm(100)
y <- rnorm(100)

t.test(x, y)

# TWO PROPORTION Z-TEST
# Is there a significant difference in proportions between two populations given normal samples of them, x and y?

x <- rnorm(100)
y <- rnorm(100)

# Assuming we want to test whether the share of values greater than zero differs between the two populations
x_count <- length(x[x>0])
y_count <- length(y[y>0])

prop.test(x=c(x_count,y_count), n=c(length(x), length(y)), alternative="two.sided")


# CHI-SQUARED TEST
# Are two variables independent? Useful for testing relationships between categorical variables

#  For example, if you were investigating the relationship between occupation and party preference, and 35% of voters were Democrats, 32% were Independents, and 33% were Republicans, independence implies the same partisan breakdown across all occupational categories (35, 32, and 33).

data_frame <- read.csv("https://goo.gl/j6lRXD")

table(data_frame$treatment, data_frame$improvement)

chisq.test(data_frame$treatment, data_frame$improvement, correct=FALSE)


# DIFFERENCE BETWEEN VARIANCES TEST
# Is the variance between two normal samples, x and y, drawn from populations significantly different?

x <- rnorm(100, sd=3)
y <- rnorm(121, sd=3.5)

var.test(x, y)
```

## Properties of Estimators

### Unbiasedness

$E(\hat{\theta}) - \theta = 0$

### Asymptotic Unbiasedness

As $n$ gets bigger, the estimator's biasedness goes to zero
$\lim_{n\to\infty} P(|E[\hat{\theta}] - \theta| > \epsilon) \to 0; \forall\epsilon >0$

### Efficiency

The estimator needs fewer observations to achieve better error
performance $\frac{1}{MSE}=\frac{1}{E[(\hat{\theta}-\theta)^2]}$

### Consistency

The distribution of estimates converges on the true value as $n$ gets
bigger
$\lim_{n\to\infty} P(|\hat{\theta} - \theta| > \epsilon) \to 0; \forall\epsilon >0$

## Correlation

We define the correlation, $r$, between two variables, $x$ and $y$, to
be:

$$r=\frac{\sum{Z_{xi}Z_{yi}}}{n-1}=\frac{\sum{(x_i-\bar{x})(y_i-\bar{y})}}{s_xs_y(n-1)}$$
where the standard deviation of $x$ (and same for $y$) is:

$$s_x=\sqrt{\frac{\sum{(x_i-\bar{x})^2}}{n-1}}$$

```{r}
x <- rnorm(n=100, mean=2, sd=3)
y <- rnorm(n=100, mean=-3, sd=2)

cor(x,y)
```

## Regression Basics

Ordinary least squares regression seeks to minimize the the vertical
distances, $\epsilon$, between the data being fitted and some line of
best fit with some slope (in the bivariate case), $\beta_1$, and some
intercept, $\beta_0$.

The model we want to fit to:

$$y_i=\beta_0+\beta_1x_i+\epsilon_i$$

The model we estimate:

$$y_i=\hat{\beta_0}+\hat{\beta_1}x_i+e_i$$ The residuals, $e_i$, are an
estimate of the error (distance between true line and y_i).

The residual sum of squares (RSS) is defined as:
$$RSS=\sum e_i^2=\sum(y_i-\hat{y_i})^2$$

```{r}
# If we have some lm model we can get the RSS as follows:
x <- rnorm(n=100, mean=2, sd=3)
y <- rnorm(n=100, mean=-3, sd=2)

model <- lm(y ~ x)

deviance(model)

# or

sum(resid(model)^2)

# Or without an lm model:

rss <- function(y, y_hat){
  return(sum((y-y_hat)^2))
}

y_hat <- fitted(model)

rss(y, y_hat)
```

The regression sum of squares (RSS) (AKA sum of squares of regression
(SSR), explained sum of squares (ESS)) is defined as:
$$RegSS=\sum(\hat{y_i}-\bar{y})^2$$

```{r}
x <- rnorm(n=100, mean=2, sd=3)
y <- rnorm(n=100, mean=-3, sd=2)

model <- lm(y ~ x)

y_hat <- fitted(model)

regSS <- function(y_hat, y){
  return(sum((y_hat-mean(y))^2))
}

regSS(y_hat, y)
```

The total sum of squares (TSS) is defined as:
$$TSS=\sum(y_i-\bar{y})^2$$

```{r}
y <- rnorm(n=100, mean=-3, sd=2)

TSS <- function(y){
  return(sum((y-mean(y))^2))
}

TSS(y)
```

The ratio of RegSS to TSS is our $r^2$, the share of variance in $y$
that is explained by $x$:
$$r^2=\frac{RegSS}{TSS}=\frac{\sum(\hat{y_i}-\bar{y})^2}{\sum(y_i-\bar{y})^2}$$

```{r}
x <- rnorm(n=100, mean=2, sd=3)
y <- rnorm(n=100, mean=-3, sd=2)

model <- lm(y ~ x)

summary(model)$r.squared

# or

y_hat <- fitted(model)

rsquared <- function(y, y_hat){
  return(sum((y_hat-mean(y))^2)/sum((y-mean(y))^2))
}

rsquared(y, y_hat)
```

### Calculating bivariate regression by hand (linear method)

#### Step 1

Given the vectors $X$ and $Y$, find $X^2$, $XY$, and the sums of all of
the preceding vectors

```{r}
calc_x2xy <- function(x, y){
  df <- data.frame(x=x,y=y)
  
  df$xsquared <- x^2
  df$xy <- x*y
  
  totals <- c(sum(x), sum(y), sum(df$xsquared), sum(df$xy))
  
  df <- rbind(df, totals)
  rownames(df) <- c(1:length(x),"total")
  
  return(df)
}

x <- rnorm(n=7, mean=2, sd=3)
y <- rnorm(n=7, mean=-3, sd=2)

df_step1 <- calc_x2xy(x=x,y=y)
df_step1
```

#### Step 2

Calculate $\hat\beta_1$ and $\hat\beta_0$:
$$\hat\beta_1=\frac{N\sum x_iy_i-\sum x_i\sum y_i}{N\sum x_i^2-(\sum x_i)^2}$$
$$\hat\beta_0=\frac{\sum y_i -\hat\beta_1\sum x_i}{N}$$

```{r}
calc_beta1 <- function(df){
  totals <- tail(df,n=1)
  N <- length(df[[1]])-1
  
  df <- head(df,n=N)
  N <- length(df$x-1) # -1 bc row totals amended
  
  beta1 <- (N*totals$xy - (totals$x*totals$y))/((N*totals$xsquared)-(totals$x)^2)
  return(beta1)
}

calc_beta0 <- function(df, beta1){
  totals <- tail(df,n=1)
  N <- length(df[[1]])-1
  
  beta0 <- (totals$y - (beta1*totals$x))/N
  return(beta0)
}

beta1 <- calc_beta1(df=df_step1)
beta1

beta0 <- calc_beta0(df=df_step1, beta1=beta1)
beta0

# check with:
# lm(y~x)
```

#### Step 3

Next, calculate $x-\bar x$, $(x- \bar x)^2$, $y-\bar y$, $(y-\bar y)^2$,
and $(x-\bar x)(y-\bar y)$.

```{r}
calc_r_table <- function(x, y){
  N <- length(x)
  df <- data.frame(x=x,y=y)
  
  df$x_minus_xbar <- df$x -  mean(x)
  df$x_minus_xbar_sq <- (df$x_minus_xbar)^2
  
  df$y_minus_ybar <- df$y -  mean(y)
  df$y_minus_ybar_sq <- (df$y_minus_ybar)^2
  
  df$x_minus_xbar_y_minus_ybar <- df$x_minus_xbar*df$y_minus_ybar
  
  return(df)
}

calc_rsquared <- function(x, y){
  df <- calc_r_table(x,y)
  N <- length(x)
  
  sd_x <- sqrt((sum(df$x_minus_xbar_sq))/(N-1))
  sd_y <- sqrt((sum(df$y_minus_ybar_sq))/(N-1))
  
  r <- (sum(df$x_minus_xbar_y_minus_ybar))/((N-1)*sd_x*sd_y)
  
  return(r^2)
}

calc_r_table(x=x,y=y)

calc_rsquared(x=x,y=y)

# check with:
#summary(lm(y~x))$r.squared
```

#### Step 4

Calculate the residuals, their square, and from that the estimated
standard error $$e_i=y_i-\hat y_i=y_i-\hat\beta_0-\hat\beta_1 x_i$$
$$\hat\sigma=\sqrt{\frac{\sum e_i^2}{N-2}}$$

```{r}
calc_residuals <- function(x, y, beta0, beta1){
  df <- data.frame(e=y-beta0-(beta1*x))
  df$e_squared <- df$e^2
  
  return(df)
}

calc_sigma <- function(e_squared, N){
  return(sqrt(sum(e_squared)/(N-2)))
}

residuals <- calc_residuals(x, y, beta0, beta1)
residuals

# check with:
#resid(lm(y~x))

sigma <- calc_sigma(residuals$e_squared, N=length(x))
sigma

# check with:
#summary(lm(y~x))$sigma


```

### Calculating bivariate regression by hand (matrix method)

Just calculate $\hat\beta=(X'X)^{-1}X'Y$

```{r}
x <- rnorm(n=7, mean=2, sd=3)
y <- rnorm(n=7, mean=-3, sd=2)

print("Append ones to X")
X <- matrix(c(rep(1, length(x)),x), nrow=length(x))
X

print("Define Y")
Y <- as.matrix(y)
Y

print("Get X'X")
t(X) %*% X

print("Get the inversion of X'X")
solve(t(X) %*% X)

print("Get X'Y")
t(X) %*% Y

print("Get beta")
solve(t(X) %*% X) %*% t(X) %*% Y

# check with:
#lm(y~x)
```

## Multiple Regression

We now have multiple $X$s we need to fit to:

$$Y=\beta_0+\beta_1X_1+\beta_2X_2+\epsilon$$

We want to find the values of $\beta_0$, $\beta_1$, and $\beta_2$ that
**minimizes** the sum of squared residuals (RSS):

$$RSS=\sum(\hat{y_i}-\bar{y})^2=\sum(\hat{y_i}-(\hat{\beta_0}+\hat\beta_1X_1+\hat\beta_2X_2)^2$$

## Categorical Variables

When regressing on a categorical variable you leave one category out.
Coefficients are the impact of being $X$ vs. the excluded category, all
else equal.

## Fixed Effects

Suppose we have data on 100 individuals in each of 100 countries
($n=10,000$).

If we include a dummy to "soak up" the country effects we can "isolate"
the differences between the individuals across countries. - The
estimated coefficients for each country typically aren't reported bc
they aren't relevant to our RQ

```{r}
data(trade)

# Without fixed effects
ols <- lm(log(Euros) ~ log(dist_km), data=trade)

summary(ols)

# With fixed effects by country of origin and destination, product, and year
fe_ols <- feols(log(Euros) ~ log(dist_km) | Origin + Destination + Product + Year, trade)

summary(fe_ols)
```

## Interactions

Case 1: Interacting a dummy with another dummy - Coefficient is the
(estimated) change in the *intercept* of the line of best fit between
units where both dummies are zero vs. where they are both 1.

Case 2: Interacting a dummy with a continuous variable - Coefficient is
the difference in coefficient on the continuous variable when dummy is
on vs. off.

Case 3: Interacting a continuous variable with another continuous
variable. - Don't try to interpret coefficient, look at an interaction
plot

Overall, an interaction tells us the *impact* one variable has on
another variable's effect on the outcome. - Only include them when you
have a theoretical reason for doing so

### Interaction plots

```{r}
set.seed(10)

#create data frame
df <- data.frame(gender = c(0,0,0,1,0,1,1,0,1,0),
                 exercise = c(0,0,2,1,2,0,1,0,2,1),
                 weight_loss = rnorm(n=10,mean=2,sd=3)) %>% 
  mutate(motivation = (2.5*weight_loss + rnorm(n=10, mean=3, sd=3)))
  
head(df)

model <- lm(weight_loss ~ gender*exercise, data=df)
summary(model)

# Plot interaction of gender*exercise
ggplot(data=df, aes(x=exercise, y=weight_loss, color=as.factor(gender))) +
  geom_point() +
  geom_smooth(method="lm") +
  labs(title="Interaction between gender and exercise") +
  theme_light()


model <- lm(weight_loss ~ exercise*motivation, data=df)
summary(model)

# To visualize an interaction between continuous variables you have to discretize
# one of them, i.e. group those above and below the mean
df$motivation_groups <- case_when(
  df$motivation > mean(df$motivation)+sd(df$motivation) ~ "high",
  df$motivation < mean(df$motivation)+sd(df$motivation) & df$motivation > mean(df$motivation)-sd(df$motivation) ~ "medium",
  df$motivation < mean(df$motivation)-sd(df$motivation) ~ "low"
)

# Can plot the interactions manually

ggplot(data=df, aes(x=exercise, y=weight_loss, color=as.factor(motivation_groups))) +
  geom_point() +
  geom_smooth(method="lm") +
  labs(title="Interaction between motivation and exercise") +
  theme_light()

# OR with plot_model
plot_model(model, type = "pred", terms = c("exercise", "motivation")) +
  theme_light()

plot_model(model, type = "int") +
  theme_light()
```

## Regression Presentation

### Regression tables

Pretty self-explanatory

### Coefficient Plots

Easier to read than a Big Ugly Table of Numbers (BUTON) - Always include
95% CI

```{r}
# Source: https://bookdown.org/paul/applied-data-visualization/graph-coefficient-plots.html

fit <- lm(Fertility ~ Catholic + Agriculture + Education, data = swiss) # see ?swiss
results <- tidy(fit)
fit_cis_95 <- confint(fit, level = 0.95) %>% 
  data.frame() %>%
  rename("conf.low_95" = "X2.5..",
         "conf.high_95" = "X97.5..")
fit_cis_90 <- confint(fit, level = 0.90) %>% 
  data.frame() %>%
  rename("conf.low_90" = "X5..",
         "conf.high_90" = "X95..")
results <- bind_cols(results, 
                     fit_cis_95, 
                     fit_cis_90) %>%
           rename(Variable = term,
                  Coefficient = estimate,
                  SE = std.error) %>%
           filter(Variable != "(Intercept)")


ggplot(results, aes(x = Variable, y = Coefficient)) +
        geom_hline(yintercept = 0, colour = gray(1/2), lty = 2) +
        geom_point(aes(x = Variable, 
                    y = Coefficient)) + 
        # geom_linerange(aes(x = Variable, 
        #              ymin = conf.low_90,
        #              ymax = conf.high_90),
        #            lwd = 1) +
        geom_linerange(aes(x = Variable, 
                     ymin = conf.low_95,
                     ymax = conf.high_95),
                   lwd = 1/2) + 
        ggtitle("Outcome: Fertility") +
        coord_flip() +
  theme_light()
```

### Predicted Value Plots

These plots convey the substantive importance of your findings; the
magnitude of the impact of your IVs on your DV

```{r}
df <- data.frame(x1=c(3, 4, 4, 5, 5, 6, 7, 8, 11, 12),
                 x2=c(6, 6, 7, 7, 8, 9, 11, 13, 14, 14),
                 x3=c(15, 12, 8, 9, 6, 5, 5, 2, 1, 0),
                 y=c(22, 24, 24, 25, 25, 27, 29, 31, 32, 36))

#fit multiple linear regression model
model <- lm(y ~ x1 + x2 +x3, data=df)

#plot predicted vs. actual values
ggplot(df, aes(x=predict(model), y=y)) + 
  geom_point() +
  geom_abline(intercept=0, slope=1) +
  labs(x='Predicted Values', y='Actual Values', 
       title='Predicted vs. Actual Values') +
  theme_light()
```

### Marginal Effects Plot

Another way to visualize the substantive meaning of your regression
coefficients, especially when interactions are involved.

```{r}
data(efc)
model <- lm(barthtot ~ c12hour + neg_c_7 * c161sex + e42dep, data = efc)

ggpredict(model, terms = "c12hour")

df_predict <- ggpredict(model, terms = "c12hour")
ggplot(df_predict, aes(x, predicted)) +
  geom_line() +
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .1)

# OR use plot_model
plot_model(model, type = "pred", terms = c("c12hour")) +
  theme_light()
```

## Regression Assumptions and Diagnostics

### Four assumptions of regression

1.  *Linearity*: The relationship between $X$ and the mean of $Y$ is
    linear

2.  *Homoscedasticity*: The variance of the residual is the same for any
    value of $X$

3.  *Independence*: Observations are independent of each other

4.  *Normality*: For any fixed value of $X$, $Y$ is normally distributed
    ## Regression Diagnostics ##\# Residual Plots Residuals are the
    variance in $Y$ that is unexplained by $X$
    $$\hat{e_i}=Y_i-\hat{Y_i}$$

```{r}
fit <- lm(Fertility ~ Catholic + Agriculture + Education, data = swiss)


# Residuals vs. X
# Ideally want a "cloud" without a large amount of values at the extremes of X, we want it to look basically random

# If we have a U-shape or an inverted U then that indicates the data may be better suited for a nonlinear model

# We also want to check for any major outliers, is there a point that's super far away from the rest?
ggplot(data=fit, aes(x=Agriculture, y=.resid)) + 
  geom_point() +
  theme_light()
```

### Leverages

Leverages tell us how much influence each value of $X$ have

```{r}
fit <- lm(Fertility ~ Catholic + Agriculture + Education, data = swiss)

leveragePlot(model=fit, term.name="Catholic")
```

### Studentized and Standardized Residuals

Standardized measurements of residuals, helps you see outliers.

```{r}
fit <- lm(Fertility ~ Catholic + Agriculture + Education, data = swiss)

# Regular ole residuals
ggplot(data=fit, aes(x=.fitted, y=.resid)) +
  geom_point() +
  theme_light()

# Studentized residuals
ggplot(data=fit, aes(x=.fitted, y=rstudent(fit))) +
  geom_point() +
  theme_light()

# Standardized residuals
ggplot(data=fit, aes(x=.fitted, y=rstandard(fit))) +
  geom_point() +
  theme_light()
```

### DFBETA

Helps assess how influential each observation is. Essentially calculates
how much the slope changes when an observation is removed from the model

```{r}
fit <- lm(Fertility ~ Catholic + Agriculture + Education, data = swiss)

ols_plot_dfbetas(fit)

# OR from car

dfbetasPlots(fit)
```

### Cook's Distance

Tells you how far, on average, the predicted y-values will move if an
observation is dropped (note DFBeta is concerned with change in slope
but also shows you outliers)

Especially helpful for multivariate regression b/c combos of $X$s could
yield an influential point a residual plot won't catch.

```{r}
fit <- lm(Fertility ~ Catholic + Agriculture + Education, data = swiss)

ggplot(data=fit, aes(x=Catholic, y=cooks.distance(fit))) +
  geom_point() +
  theme_light()

ggplot(data=fit, aes(x=Agriculture, y=cooks.distance(fit))) +
  geom_point() +
  theme_light()
```

### Multicollinearity

The symptoms:

-   Instability of estimates when adding or dropping variables

-   Massive coefficients or t-statistics

-   Two or more variables that are similar but with opposite signs

What do to:

-   More data!

-   Simplify the model

    -   Do we need all the variables that are collinear?

    -   Could we combine two or more variables to create some sort of
        index?

# POLI 271

## Likelihood Theory

### Deriving the log likelihood

Given some probability density function (our stochastic component),
$f(y;\theta)$ (could be normal, Gaussian, etc.):

1.  Get the joint probability of the data. If the probabilities are
    independent then we have:

$$Pr(y_i,. . ., y_n|\theta_1, . . ., \theta_n)=Pr(y;\theta)=\prod_{i=1}^nf(y_i;\theta_i)$$

2.  Convert the joint probability to a likelihood. Take the joint
    probability, multiply it by some constant scalar, $h(y)$.

$$\mathcal{L}(\theta|y)=h(y)\times Pr(y;\theta)$$

3.  Plug in our $f(y;\theta)$ and simplify $\mathcal{L}(\theta|y)$

4.  Simplify further by taking the log of $\mathcal{L}(\theta|y)$

5.  Find the critical points by taking the first derivative w.r.t
    $\theta$

6.  Check whether they are maxima or minima by taking the second
    derivative w.r.t. $\theta$

An example:

The exponential probability density function is defined as:
$f_e(x;\theta)=\theta e^{-\theta x}$.

First, note that since $X~f_e(x;\theta)$ the support (possible values of
$X$) is $\mathbb{R}\in [0, \infty]$

1.  Get the joint probability

$$f_{\mathrm{joint}}(x,\theta)=\prod_{i=1}^n(\theta e^{-\theta x_i})=\theta^n\prod_{i=1}^n( e^{-\theta x_i})$$

2.  Define the likelihood function

$$\mathcal{L}(\theta|x)=h(x)\theta^n\prod_{i=1}^n( e^{-\theta x_i})$$

Let $h(x)=1$

$$\mathcal{L}(\theta|x)=\theta^n\prod_{i=1}^n( e^{-\theta x_i})$$

3.  Take the log

$$\ln(\mathcal{L}(\theta|x))=\ln(\theta^n\prod_{i=1}^n( e^{-\theta x_i}))$$
$$\ln(\mathcal{L}(\theta|x))=n\ln(\theta)-\theta\sum_{i=1}^nx_i$$

4.  Take the first derivative and identify the critical points

$$\frac{d\ln(\mathcal{L}(\theta|x))}{d\theta}=\frac{n}{\theta}-\sum_{i=1}^nx_i$$

Set the derivative equal to zero to find the critical values of
$\theta$:

$$\frac{n}{\theta}-\sum_{i=1}^nx_i=0$$

$$\hat{\theta}=\frac{n}{\sum_{i=1}^nx_i}$$

5.  Take the second derivative and substitute $\hat{\theta}$ for
    $\theta$. If the result is positive the critical point is a minumum,
    if zero then it's a saddle, if negative it's a maximum.

$$\frac{d^2\ln(\mathcal{L}(\theta|x))}{d\theta^2}=\frac{-n}{\theta^2}$$

Plugging in $\hat{\theta}$:

$$\frac{d^2ln(\mathcal{L}(\theta=\hat{\theta}|x))}{d\theta^2}=\frac{-n\sum_{i=1}^nx_i}{n}=-\sum_{i=1}^nx_i$$

What's the sign of $-\sum_{i=1}^nx_i$?

We know $-\sum_{i=1}^nx_i<0$ because $\sum_{i=1}^nx_i>0$ given the $X$
is bounded between $[0, \infty]$ therefore it is a maximum and thus
$\hat{\theta}$ is a maximum and the MLE is
$\hat{\theta}=\frac{n}{\sum_{i=1}^nx_i}$.

Now in R:

```{r}
# Sets the seed for R's baked in random number generation for easy replication
set.seed(2)

# Assigns x to be a vector of 1000 observations drawn from the exponential distribution
# with lambda=3
x <- rexp(1000, 3)

# Returns the log likelihood for some parameter, theta, given data, x
exp.ll <- function(theta, x){return((length(x)*log(theta))-(theta*sum(x)))}

thetas = seq(0,25,0.1)

y <- c()

for(theta in thetas){
  y <- c(y,exp.ll(theta=theta,x=x))
}

theta_hat <- length(x)/sum(x)
paste("For the given x, theta_hat is: ", theta_hat)

df <- data.frame(y,thetas)

ggplot(data=df,aes(x=thetas,y=y)) +
  geom_point(size=0.5) +
  geom_vline(xintercept=theta_hat, color="red", linetype="dashed") +
  ggtitle("Log likelihood vs. theta") +
  xlab("theta") +
  ylab("Log likelihood") +
  theme_bw()
```

Note: See MLE Problem Set 2 for a derivation of the MLE of the normal
variance.

## Regularity

In order to find the MLE as we did above, the likelihood function must
demonstrate "regularity", i.e. it must be continuous and twice
differentiable.

## Binary Data

### Why not just use OLS?

-   Nonsensical predictions: predicted probabilites using a linear
    probability model on a binary outcome can fall outside $[0,1]$,
    meaning the OLS estimate is biased and inconsistent.
-   Assumes the relationship between a covariate and the probability of
    1 as the outcome is constant across all values of the covariate
-   Can give you heteroskedastic residuals that give biased confidence
    intervals

### The Logit Model

Logit allows for mapping probabilities onto the unbounded real line
using odds. We transform a probability, $p$, to logit by taking the log
of the odds ratio:

$$\mathrm{logit}(p)=\log(\frac{p}{1-p})$$

And the inverse mapping:\

$$
\mathrm{logit}^{-1}(x)=\frac{e^x}{1+e^x}=\frac{1}{1+e^{-x}}
$$

We model binary outcomes as Bernoulli trials so our stochastic component
is:

$$
f_B(y_i;\theta_i)=\theta_i^{y_i}(1-\theta_i)^{1-y_i}= 
   \left\{
\begin{array}{ll}
      \theta_i & y_i=1 \\
      1-\theta_i & y_i=0
\end{array} 
\right. 
$$

And our systematic component is:

$$
\theta_i=\mathrm{logit}^{-1}(x_i^T\beta)=\frac{1}{1+e^{-x_i^T\beta}}
$$

As we do above we can then get the joint probability distribution:

$$
Pr(y|\theta)=\prod_{i=1}^n \theta_i^{y_i}(1-\theta_i)^{1-y_i}
$$

And the log-likelihood:

$$
\log\mathcal{L}(\theta|y)=\sum_{i=1}^n\log(\frac{e^{-x_i^T\beta^{1-y_i}}}{1+e^{-x_i^T\beta}})
$$

Finding the MLE with R (Ward and Ahlquist code):

```{r}
#function for optimization of logit
binreg<- function(X,y,method="BFGS"){
  X<- cbind(1,X)
  
  # Get the log-likelihood
  negLL<- function(b,X,y){
    p<-as.vector(1/(1+exp(-X %*% b)))
    - sum(y*log(p) + (1-y)*log(1-p))
  }
  
  # Get the gradient
  gradient<- function(b,X,y){
    p <- as.vector(1/(1+exp(-X %*% b))) - 
      apply(((y - p)*X),
            2,
            sum)
  }
  
  # Pass our data, log-likelihood, and gradient to optim() 
  # which will find the maximum likelihood
  results<- optim(par=rep(0,ncol(X)), 
                  fn=negLL,
                  gr=gradient, 
                  hessian=T, 
                  method=method,
                  X=X,
                  y=y)
  
  # Pull out the stuff we want
  list(coefficients=results$par,
       var=solve(results$hessian),
       deviance=2*results$value,
       converged=results$convergence==0)
}

data("mtcars")

# set the DV to the binary variable for V-shaped vs. straight engine type
DV <- mtcars$vs

IVs <- as.matrix(mtcars %>% select(cyl, disp, hp))

mlebin.fit<-binreg(X=IVs, y=DV)

# Get our logit coefficients
round(mlebin.fit$coefficients,2)


# Compare to R's baked in logit estimation
fit.glm <- glm(vs ~ cyl + disp + hp,
               family = binomial(link = "logit"), 
               data = mtcars)
summary(fit.glm, signif.stars=FALSE)
```

### Interpreting Logit

While the output of logit is similar to OLS, interpretation is a bit
more nuanced for 2 reasons:

1.  The model is nonlinear so the effect of a particular covariate on
    the DV is not constant across all levels of the covariate. This can
    be seen by deriving the marginal effect of a covariate, $x_k$, on
    the expected value of $Y$:

    $$\frac{\partial E[Y_i]}{\partial x_{ki}}=\frac{\partial\theta_i}{\partial x_{ki}}=\beta_k\frac{e^{x_i^T\beta}}{(1+e^{x_i^T\beta)^2}$$

    As a quick trick, you can divide a coefficient by 4 to get an
    estimate of a maximum effect a 1 unit increase in the covariate can
    have on the probability of the DV being 1. i.e. if we have a
    coefficent of -1.45 then $\frac{-1.45}{4}=-0.4$ so a 1 unit increase in the IV reduces the probability of the DV being one by about 40%.

2.  The logit model is a linear regression on the log odds so the
    exponentiated coefficients are odds ratios. If we have a coefficient
    greater than 1 then a 1 unit increase in that covariate corresponds
    to an *increase* in the relative probability of obtaining a 1 in the
    outcome variable. Less than 1 represents a decrease.
    
### Interpreting Logit with Plots

```{r}
data("mtcars")

# Estimate the model
model <- glm(vs ~ cyl + disp + hp, family = binomial(link = "logit"), data = mtcars)


```


## Out-of-sample Prediction

## Model Selection

## Model Interpretation

# 2021 Exam Questions and Answers
